{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'D:\\School\\GitHub\\SMU_MSDS_Capstone\\Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining DF's that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all the CSV files in that folder\n",
    "all_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f)) and f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to hold the dataframes\n",
    "list_of_dfs = []\n",
    "\n",
    "# Loop through each CSV file and read it into a DataFrame, then append to the list\n",
    "for file in all_files:\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        df = pd.read_csv(f)\n",
    "    list_of_dfs.append(df)\n",
    "\n",
    "# Concatenate all the dataframes together\n",
    "combined_df = pd.concat(list_of_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.head()) # Just print the first few rows as a check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After combining all your DataFrames into combined_df...\n",
    "output_file_path = 'D:\\School\\GitHub\\SMU_MSDS_Capstone\\Output1.csv'\n",
    "combined_df.to_csv(output_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling a list of DOI's to grab information for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_list = combined_df['DOI'].unique().tolist()\n",
    "doi_df = pd.DataFrame(doi_list, columns=['DOI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "\n",
    "# Initialize a list to store all the fetched DOIs\n",
    "all_dois = []\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(subjects_list), desc=\"Fetching DOIs\")\n",
    "\n",
    "# Step 2: Iterate through each subject title and fetch 100 unique DOIs\n",
    "for subject_name in subjects_list:\n",
    "    doi_url = f\"https://api.crossref.org/works?query=subject:{subject_name}&rows=100\"\n",
    "    response = requests.get(doi_url)\n",
    "    data = response.json()\n",
    "    subject_dois = data.get('message', {}).get('items', [])\n",
    "    \n",
    "    # Add the fetched DOIs to the all_dois list\n",
    "    all_dois.extend(subject_dois)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "    \n",
    "    # Break the loop if we have fetched 25 unique DOIs for each subject\n",
    "    if len(all_dois) >= 25 * len(subjects_list):\n",
    "        break\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "temp_doi_df = pd.DataFrame(all_dois, columns=['DOI'])\n",
    "doi_df = pd.concat([doi_df, temp_doi_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doi_df.drop_duplicates(inplace=True)\n",
    "doi_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doi_df.describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Info From The DOI's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_details(doi, cache):\n",
    "    # If the details for the DOI are already in the cache\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() \n",
    "            data = response.json()\n",
    "            message = data['message']\n",
    "\n",
    "            # Extract common details\n",
    "            title = message['title'][0] if 'title' in message else None\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])[0]\n",
    "            field = message.get('subject', [None])[0]\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "\n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal,\n",
    "                'Field': field,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "\n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Unexpected structure in response for DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "# Assuming you have the DataFrame named combined_df\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "data_list = []\n",
    "\n",
    "for idx, row in tqdm(doi_df.iterrows(), total=len(doi_df), desc=\"Fetching details\"):\n",
    "    if pd.notna(row['DOI']):  \n",
    "        details = fetch_details(row['DOI'], doi_cache)\n",
    "        data_list.extend(details)  # Since details is now a list of dictionaries\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Append new_data_df to full_df\n",
    "full_df = pd.concat([full_df, new_data_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df['DOI'].nunique())\n",
    "print(full_df.shape[0])\n",
    "\n",
    "# Assuming your DataFrame is named 'full_df'\n",
    "subset_columns = ['DOI', 'Author First Name', 'Author Last Name']\n",
    "\n",
    "# Group by DOI and apply a function to remove duplicates within each group\n",
    "deduplicated_df = full_df.groupby('DOI', group_keys=False).apply(\n",
    "    lambda group: group.drop_duplicates(subset=subset_columns)\n",
    ")\n",
    "\n",
    "# Print the number of rows before and after deduplication\n",
    "print(f\"Number of rows before deduplication: {len(full_df)}\")\n",
    "print(f\"Number of rows after deduplication: {len(deduplicated_df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
