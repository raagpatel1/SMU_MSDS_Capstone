{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pull the unniversity as well\n",
    "\n",
    "chatgpt where it is\n",
    "\n",
    "add 2017-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "subjects = subjects_list\n",
    "rows = 1000\n",
    "\n",
    "# Define the years you want to pull data for\n",
    "years_to_pull = [2017, 2018, 2019, 2020, 2021, 2022, 2023]  \n",
    "\n",
    "def get_articles_by_subject(subject, year, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows,\n",
    "        'query.bibliographic': subject\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    cursor = '*'\n",
    "\n",
    "    while cursor:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor', '')\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "# Initialize an empty list to store DataFrames for each subject\n",
    "subject_dataframes = []\n",
    "\n",
    "# Loop through each subject and fetch data  \n",
    "for subject in subjects_list:\n",
    "    try:\n",
    "        all_articles = []\n",
    "        for year in years_to_pull:\n",
    "            articles = get_articles_by_subject(subject, year, rows=rows)\n",
    "            all_articles.extend(articles)\n",
    "\n",
    "            data = {\n",
    "                'DOI': [],\n",
    "                'Title': [],\n",
    "                'Container Title': [],\n",
    "                'Publisher': [],\n",
    "                'Publish Date': [],\n",
    "                'Author First Name': [],\n",
    "                'Author Last Name': [],\n",
    "                'Author Order': [],\n",
    "                'Referenced By': []\n",
    "            }\n",
    "\n",
    "            for article in all_articles:\n",
    "                doi = article.get('DOI', '')\n",
    "                title = article.get('title', [''])[0]\n",
    "                container_title = article.get('container-title', [''])[0]\n",
    "                publisher = article.get('publisher', '')\n",
    "                publish_date = article.get('published-print', {}).get('date-parts', [[]])[0]\n",
    "                authors = article.get('author', [])\n",
    "                referenced_by = article.get('is-referenced-by-count', 0)\n",
    "\n",
    "                for order, author in enumerate(authors, start=1):\n",
    "                    first_name = author.get('given', '')\n",
    "                    last_name = author.get('family', '')\n",
    "\n",
    "                    data['DOI'].append(doi)\n",
    "                    data['Title'].append(title)\n",
    "                    data['Container Title'].append(container_title)\n",
    "                    data['Publisher'].append(publisher)\n",
    "                    data['Publish Date'].append(publish_date)\n",
    "                    data['Author First Name'].append(first_name)\n",
    "                    data['Author Last Name'].append(last_name)\n",
    "                    data['Author Order'].append(order)\n",
    "                    data['Referenced By'].append(referenced_by)\n",
    "\n",
    "            subject_df = pd.DataFrame(data)\n",
    "            subject_df.columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date', 'Author First Name', 'Author Last Name', 'Author Order', 'Referenced By']\n",
    "            subject_df['Subject'] = subject  # Add a Subject column with the current subject\n",
    "            subject_dataframes.append(subject_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing subject '{subject}': {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all DataFrames into one big DataFrame\n",
    "massive_Crossreff_pull = pd.concat(subject_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "massive_Crossreff_pull.to_csv('massive_Crossreff_pull.csv', index=False)\n",
    "massive_Crossreff_pull.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5997846, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "massive_Crossreff_pull.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Titles: 651057\n"
     ]
    }
   ],
   "source": [
    "# Count the unique titles\n",
    "unique_title_count = massive_Crossreff_pull['Title'].nunique()\n",
    "print(f\"Unique Titles: {unique_title_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Unique Author Counts per Title:\n",
      "count    651057.000000\n",
      "mean          3.184300\n",
      "std           3.680977\n",
      "min           1.000000\n",
      "25%           2.000000\n",
      "50%           3.000000\n",
      "75%           4.000000\n",
      "max        1566.000000\n",
      "Name: Unique Author Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "title_author_counts = massive_Crossreff_pull.groupby('Title')['Author Last Name'].nunique().reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "title_author_counts.columns = ['Title', 'Unique Author Count']\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "statistics = title_author_counts['Unique Author Count'].describe()\n",
    "\n",
    "# Print the results\n",
    "print(\"Descriptive Statistics for Unique Author Counts per Title:\")\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Statistics for Unique Title Counts per Subject:\n",
      "count     234.000000\n",
      "mean     3088.423077\n",
      "std       574.991364\n",
      "min       519.000000\n",
      "25%      2851.000000\n",
      "50%      3264.500000\n",
      "75%      3490.500000\n",
      "max      3907.000000\n",
      "Name: Unique Title Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by subject and count unique titles in each subject\n",
    "subject_title_counts = massive_Crossreff_pull.groupby('Subject')['Title'].nunique().reset_index()\n",
    "subject_title_counts.columns = ['Subject', 'Unique Title Count']\n",
    "\n",
    "# Calculate descriptive statistics for unique title counts per subject\n",
    "subject_stats = subject_title_counts['Unique Title Count'].describe()\n",
    "\n",
    "# Print the descriptive statistics for unique title counts per subject\n",
    "print(\"Descriptive Statistics for Unique Title Counts per Subject:\")\n",
    "print(subject_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Value Counts for Each Column:\n",
      "DOI                  0\n",
      "Title                0\n",
      "Container Title      0\n",
      "Publisher            0\n",
      "Publish Date         0\n",
      "Author First Name    0\n",
      "Author Last Name     0\n",
      "Author Order         0\n",
      "Referenced By        0\n",
      "Subject              0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the null values in each column\n",
    "null_counts = massive_Crossreff_pull.isnull().sum()\n",
    "\n",
    "# Print the null counts for each column\n",
    "print(\"Null Value Counts for Each Column:\")\n",
    "print(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Character Count in 'Author First Name' column: 37486849\n",
      "Average Character Count in 'Author First Name' column per row: 6.25\n",
      "Number of Non-Null Values in 'Author First Name' column: 5997846\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count of characters in the 'Author First Name' column for each row\n",
    "total_character_count = massive_Crossreff_pull['Author First Name'].str.len().sum()\n",
    "\n",
    "# Calculate the average number of characters in the 'Author First Name' column per row\n",
    "average_character_count = massive_Crossreff_pull['Author First Name'].str.len().mean()\n",
    "\n",
    "# Calculate the unique count of characters in the 'Author First Name' column\n",
    "non_null_count = massive_Crossreff_pull['Author First Name'].count()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total Character Count in 'Author First Name' column: {total_character_count}\")\n",
    "print(f\"Average Character Count in 'Author First Name' column per row: {average_character_count:.2f}\")\n",
    "print(f\"Number of Non-Null Values in 'Author First Name' column: {non_null_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "\n",
    "# Initialize rows and years to pull\n",
    "rows = 1\n",
    "years_to_pull = [2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "\n",
    "# Function to get articles by journal name and year\n",
    "def get_articles_by_journal(journal_name, year, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows,\n",
    "        'query.container-title': journal_name\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        articles = data.get('message', {}).get('items', [])\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(response.content)\n",
    "    return articles\n",
    "\n",
    "# Initialize DataFrame\n",
    "df_columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date',\n",
    "              'Author First Name', 'Author Last Name', 'Author Order', 'Affiliation', 'Country', 'Referenced By']\n",
    "data = {col: [] for col in df_columns}\n",
    "\n",
    "# Fetch articles\n",
    "for year in years_to_pull:\n",
    "    articles = get_articles_by_journal(\"Nature\", year, rows=rows)\n",
    "    \n",
    "    for article in articles:\n",
    "        # Populate existing and new attributes here\n",
    "        doi = article.get('DOI', '')\n",
    "        title = article.get('title', [''])[0]\n",
    "        container_title = article.get('container-title', [''])[0]\n",
    "        publisher = article.get('publisher', '')\n",
    "        \n",
    "        # Get date\n",
    "        publish_date_parts = article.get('published-print', {}).get('date-parts', [[]])[0]\n",
    "        if len(publish_date_parts) == 3:\n",
    "            publish_date = date(*publish_date_parts)\n",
    "        elif len(publish_date_parts) == 2:\n",
    "            publish_date = date(publish_date_parts[0], publish_date_parts[1], 1)\n",
    "        elif len(publish_date_parts) == 1:\n",
    "            publish_date = date(publish_date_parts[0], 1, 1)\n",
    "        else:\n",
    "            publish_date = None\n",
    "        \n",
    "        referenced_by = article.get('is-referenced-by-count', 0)\n",
    "        authors = article.get('author', [])\n",
    "        \n",
    "        for order, author in enumerate(authors, start=1):\n",
    "            first_name = author.get('given', '')\n",
    "            last_name = author.get('family', '')\n",
    "            affiliation_list = author.get('affiliation', [])\n",
    "            if affiliation_list:\n",
    "                affiliation = affiliation_list[0].get('name', '')\n",
    "                country = affiliation_list[0].get('country', '')\n",
    "            else:\n",
    "                affiliation = ''\n",
    "                country = ''\n",
    "\n",
    "        \n",
    "            for col, value in zip(df_columns, [doi, title, container_title, publisher, publish_date, \n",
    "                                               first_name, last_name, order, affiliation, country, referenced_by]):\n",
    "                data[col].append(value)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "df['Year'] = df['Publish Date'].apply(lambda x: x.year if x else None)\n",
    "\n",
    "# Save DataFrame\n",
    "df.to_csv('Nature_articles.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
