{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    # If the details for the DOI are already in the cache\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status() \n",
    "            data = response.json()\n",
    "            message = data['message']\n",
    "\n",
    "            # Extract common details\n",
    "            title = message['title'][0] if 'title' in message else None\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])[0]\n",
    "            field = message.get('subject', [None])[0]\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "\n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal,\n",
    "                'Field': field,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "\n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Unexpected structure in response for DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "\n",
    "# Initialize a DataFrame to store DOIs\n",
    "doi_df = pd.DataFrame(columns=['DOI'])\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(subjects_list), desc=\"Fetching DOIs\")\n",
    "\n",
    "# Step 2: Iterate through each subject title and fetch 25 unique DOIs\n",
    "for subject_name in subjects_list:\n",
    "    doi_url = f\"https://api.crossref.org/works?query=subject:{subject_name}&rows=25\"\n",
    "    response = requests.get(doi_url)\n",
    "    data = response.json()\n",
    "    subject_dois = [item['DOI'] for item in data.get('message', {}).get('items', [])]\n",
    "    \n",
    "    # Add the fetched DOIs to the doi_df DataFrame\n",
    "    temp_doi_df = pd.DataFrame(subject_dois, columns=['DOI'])\n",
    "    doi_df = pd.concat([doi_df, temp_doi_df], ignore_index=True)\n",
    "    \n",
    "    # Fetch details for each DOI and populate the full_df DataFrame\n",
    "    data_list = []\n",
    "    for doi in subject_dois:\n",
    "        details = fetch_details(doi, doi_cache)\n",
    "        data_list.extend(details)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    new_data_df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Break the loop if we have fetched details for all DOIs\n",
    "    if len(doi_df) >= 25 * len(subjects_list):\n",
    "        break\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Display the resulting full_df DataFrame with DOI details\n",
    "print(new_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    if doi in cache:\n",
    "        return cache[doi]\n",
    "\n",
    "    url = f\"https://api.crossref.org/works/{doi}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        message = data.get('message', {})\n",
    "\n",
    "        title = message.get('title', [None])[0]\n",
    "        abstract = message.get('abstract', None)\n",
    "        journal = message.get('container-title', [None])[0]\n",
    "        field = message.get('subject', [None])[0]\n",
    "        citation_count = message.get('is-referenced-by-count', None)\n",
    "        date_received = message.get('created', {}).get('date-time', None)\n",
    "        date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "        address = message.get('publisher-location', None)\n",
    "        language = message.get('language', None)\n",
    "\n",
    "        common_details = {\n",
    "            'DOI': doi,\n",
    "            'Title': title,\n",
    "            'Abstract': abstract,\n",
    "            'Journal': journal,\n",
    "            'Field': field,\n",
    "            'Citation Count': citation_count,\n",
    "            'Date Received': date_received,\n",
    "            'Date Published': date_published,\n",
    "            'Address': address,\n",
    "            'Language': language\n",
    "        }\n",
    "\n",
    "        authors = message.get('author', [])\n",
    "        details = {'common': common_details, 'authors': authors}\n",
    "        cache[doi] = details\n",
    "        return details\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching DOI {doi}: {e}\")\n",
    "        return {'common': None, 'authors': []}\n",
    "    except (KeyError, IndexError) as e:\n",
    "        print(f\"Unexpected structure in response for DOI {doi}: {e}\")\n",
    "        return {'common': None, 'authors': []}\n",
    "\n",
    "\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "subjects_list = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_list.update(item['subject'])\n",
    "\n",
    "subjects_list = list(subjects_list)\n",
    "doi_df = pd.DataFrame(columns=['DOI'])\n",
    "doi_cache = {}\n",
    "pbar = tqdm(total=len(subjects_list), desc=\"Fetching DOIs\")\n",
    "\n",
    "for subject_name in subjects_list:\n",
    "    doi_url = f\"https://api.crossref.org/works?query=subject:{subject_name}&rows=25\"\n",
    "    response = requests.get(doi_url)\n",
    "    data = response.json()\n",
    "    subject_dois = [item['DOI'] for item in data.get('message', {}).get('items', [])]\n",
    "\n",
    "    temp_doi_df = pd.DataFrame(subject_dois, columns=['DOI'])\n",
    "    doi_df = pd.concat([doi_df, temp_doi_df], ignore_index=True)\n",
    "\n",
    "    data_list = []\n",
    "    for doi in subject_dois:\n",
    "        details = fetch_details(doi, doi_cache)\n",
    "        if details['common']:\n",
    "            data_list.extend(details)\n",
    "\n",
    "    new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "    if len(doi_df) >= 25 * len(subjects_list):\n",
    "        break\n",
    "\n",
    "pbar.close()\n",
    "print(new_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'indexed': {'date-parts': [[2022, 3, 31]], 'date-time': '2022-03-31T15:25:27Z', 'timestamp': 1648740327902}, 'publisher-location': 'London', 'standards-body': {'name': 'BSI British Standards', 'acronym': 'BSI'}, 'reference-count': 0, 'publisher': 'BSI British Standards', 'content-domain': {'domain': [], 'crossmark-restriction': False}, 'short-container-title': [], 'DOI': '10.3403/30087604', 'type': 'standard', 'created': {'date-parts': [[2013, 11, 13]], 'date-time': '2013-11-13T16:07:16Z', 'timestamp': 1384358836000}, 'approved': {'date-parts': [[2008, 12, 31]]}, 'source': 'Crossref', 'is-referenced-by-count': 1, 'title': ['Building and civil engineering. Vocabulary'], 'prefix': '10.3403', 'member': '1988', 'container-title': [], 'original-title': [], 'deposited': {'date-parts': [[2017, 12, 5]], 'date-time': '2017-12-05T07:43:55Z', 'timestamp': 1512459835000}, 'score': 1, 'resource': {'primary': {'URL': 'https://linkresolver.bsigroup.com/junction/resolve/000000000030087604?restype=standard'}}, 'subtitle': ['Civil engineering. Water engineering, environmental engineering and pipe lines'], 'short-title': [], 'issued': {'date-parts': [[None]]}, 'references-count': 0, 'URL': 'http://dx.doi.org/10.3403/30087604', 'relation': {}}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# DOI to fetch details for\n",
    "doi = \"10.3403/30087604\"\n",
    "\n",
    "# URL to fetch details from CrossRef API\n",
    "url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "# Fetch the data\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Display the entire message\n",
    "print(data['message'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# DOI to fetch details for\n",
    "doi = \"10.5194/angeo-2018-95-rc4\"\n",
    "\n",
    "# URL to fetch details from CrossRef API\n",
    "url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "# Fetch the data\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Extracting common details\n",
    "message = data['message']\n",
    "title = message['title'][0] if 'title' in message else None\n",
    "abstract = message.get('abstract', None)\n",
    "journal = message.get('container-title', [None])[0]\n",
    "field = message.get('subject', [None])[0]\n",
    "citation_count = message.get('is-referenced-by-count', None)\n",
    "date_received = message.get('created', {}).get('date-time', None)\n",
    "date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "address = message.get('publisher-location', None)\n",
    "language = message.get('language', None)\n",
    "\n",
    "# Display the extracted details\n",
    "print(\"Title:\", title)\n",
    "print(\"Abstract:\", abstract)\n",
    "print(\"Journal:\", journal)\n",
    "print(\"Field:\", field)\n",
    "print(\"Citation Count:\", citation_count)\n",
    "print(\"Date Received:\", date_received)\n",
    "print(\"Date Published:\", date_published)\n",
    "print(\"Address:\", address)\n",
    "print(\"Language:\", language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# DOI to fetch details for\n",
    "doi = \"10.5194/angeo-2018-95-rc4\"\n",
    "\n",
    "# URL to fetch details from CrossRef API\n",
    "url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "# Fetch the data\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Accessing specific fields and handling empty values\n",
    "indexed_date = message.get('indexed', {}).get('date-time', None)\n",
    "posted_date = message.get('posted', {}).get('date-parts', [])[0] if 'posted' in message else None\n",
    "publisher = message.get('publisher', None)\n",
    "reference_count = message.get('reference-count', None)\n",
    "content_domain = message.get('content-domain', {}).get('domain', [])\n",
    "short_container_title = message.get('short-container-title', [])\n",
    "type_ = message.get('type', None)\n",
    "created_date = message.get('created', {}).get('date-time', None)\n",
    "source = message.get('source', None)\n",
    "is_referenced_by_count = message.get('is-referenced-by-count', None)\n",
    "title = message.get('title', [])\n",
    "prefix = message.get('prefix', None)\n",
    "authors = message.get('author', [])\n",
    "member = message.get('member', None)\n",
    "container_title = message.get('container-title', [])\n",
    "deposited_date = message.get('deposited', {}).get('date-time', None)\n",
    "score = message.get('score', None)\n",
    "resource_url = message.get('resource', {}).get('primary', {}).get('URL', None)\n",
    "subtitle = message.get('subtitle', [])\n",
    "short_title = message.get('short-title', [])\n",
    "issued_date = message.get('issued', {}).get('date-parts', [])[0] if 'issued' in message else None\n",
    "references_count = message.get('references-count', None)\n",
    "url = message.get('URL', None)\n",
    "relation_is_reply_to = message.get('relation', {}).get('is-reply-to', [])\n",
    "relation_has_reply = message.get('relation', {}).get('has-reply', [])\n",
    "published_date = message.get('published', {}).get('date-parts', [])[0] if 'published' in message else None\n",
    "subtype = message.get('subtype', None)\n",
    "\n",
    "# Displaying the extracted fields\n",
    "print(\"Indexed Date:\", indexed_date)\n",
    "print(\"Posted Date:\", posted_date)\n",
    "print(\"Publisher:\", publisher)\n",
    "print(\"Reference Count:\", reference_count)\n",
    "print(\"Content Domain:\", content_domain)\n",
    "print(\"Short Container Title:\", short_container_title)\n",
    "print(\"Type:\", type_)\n",
    "print(\"Created Date:\", created_date)\n",
    "print(\"Source:\", source)\n",
    "print(\"Is Referenced By Count:\", is_referenced_by_count)\n",
    "print(\"Title:\", title)\n",
    "print(\"Prefix:\", prefix)\n",
    "print(\"Authors:\", authors)\n",
    "print(\"Member:\", member)\n",
    "print(\"Container Title:\", container_title)\n",
    "print(\"Deposited Date:\", deposited_date)\n",
    "print(\"Score:\", score)\n",
    "print(\"Resource URL:\", resource_url)\n",
    "print(\"Subtitle:\", subtitle)\n",
    "print(\"Short Title:\", short_title)\n",
    "print(\"Issued Date:\", issued_date)\n",
    "print(\"References Count:\", references_count)\n",
    "print(\"URL:\", url)\n",
    "print(\"Relation (is reply to):\", relation_is_reply_to)\n",
    "print(\"Relation (has reply):\", relation_has_reply)\n",
    "print(\"Published Date:\", published_date)\n",
    "print(\"Subtype:\", subtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'message' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n\u001b[0;32m     13\u001b[0m \u001b[39m# Accessing specific fields and handling empty values\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m indexed_date \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mindexed\u001b[39m\u001b[39m'\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdate-time\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m posted_date \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mposted\u001b[39m\u001b[39m'\u001b[39m, {})\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdate-parts\u001b[39m\u001b[39m'\u001b[39m, [])[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mposted\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m message \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     16\u001b[0m publisher \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mpublisher\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'message' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# DOI to fetch details for\n",
    "doi = \"10.3403/30087604\"\n",
    "\n",
    "# URL to fetch details from CrossRef API\n",
    "url = f\"https://api.crossref.org/works/{doi}\"\n",
    "\n",
    "# Fetch the data\n",
    "response = requests.get(url)\n",
    "data = response.json()\n",
    "\n",
    "# Accessing specific fields and handling empty values\n",
    "indexed_date = message.get('indexed', {}).get('date-time', None)\n",
    "posted_date = message.get('posted', {}).get('date-parts', [])[0] if 'posted' in message else None\n",
    "publisher = message.get('publisher', None)\n",
    "reference_count = message.get('reference-count', None)\n",
    "content_domain = message.get('content-domain', {}).get('domain', [])\n",
    "short_container_title = message.get('short-container-title', [])\n",
    "type_ = message.get('type', None)\n",
    "created_date = message.get('created', {}).get('date-time', None)\n",
    "source = message.get('source', None)\n",
    "is_referenced_by_count = message.get('is-referenced-by-count', None)\n",
    "title = message.get('title', [])\n",
    "prefix = message.get('prefix', None)\n",
    "authors = message.get('author', [])\n",
    "member = message.get('member', None)\n",
    "container_title = message.get('container-title', [])\n",
    "deposited_date = message.get('deposited', {}).get('date-time', None)\n",
    "score = message.get('score', None)\n",
    "resource_url = message.get('resource', {}).get('primary', {}).get('URL', None)\n",
    "subtitle = message.get('subtitle', [])\n",
    "short_title = message.get('short-title', [])\n",
    "issued_date = message.get('issued', {}).get('date-parts', [])[0] if 'issued' in message else None\n",
    "references_count = message.get('references-count', None)\n",
    "url = message.get('URL', None)\n",
    "relation_is_reply_to = message.get('relation', {}).get('is-reply-to', [])\n",
    "relation_has_reply = message.get('relation', {}).get('has-reply', [])\n",
    "published_date = message.get('published', {}).get('date-parts', [])[0] if 'published' in message else None\n",
    "subtype = message.get('subtype', None)\n",
    "\n",
    "# Displaying the extracted fields\n",
    "print(\"Indexed Date:\", indexed_date)\n",
    "print(\"Posted Date:\", posted_date)\n",
    "print(\"Publisher:\", publisher)\n",
    "print(\"Reference Count:\", reference_count)\n",
    "print(\"Content Domain:\", content_domain)\n",
    "print(\"Short Container Title:\", short_container_title)\n",
    "print(\"Type:\", type_)\n",
    "print(\"Created Date:\", created_date)\n",
    "print(\"Source:\", source)\n",
    "print(\"Is Referenced By Count:\", is_referenced_by_count)\n",
    "print(\"Title:\", title)\n",
    "print(\"Prefix:\", prefix)\n",
    "print(\"Authors:\", authors)\n",
    "print(\"Member:\", member)\n",
    "print(\"Container Title:\", container_title)\n",
    "print(\"Deposited Date:\", deposited_date)\n",
    "print(\"Score:\", score)\n",
    "print(\"Resource URL:\", resource_url)\n",
    "print(\"Subtitle:\", subtitle)\n",
    "print(\"Short Title:\", short_title)\n",
    "print(\"Issued Date:\", issued_date)\n",
    "print(\"References Count:\", references_count)\n",
    "print(\"URL:\", url)\n",
    "print(\"Relation (is reply to):\", relation_is_reply_to)\n",
    "print(\"Relation (has reply):\", relation_has_reply)\n",
    "print(\"Published Date:\", published_date)\n",
    "print(\"Subtype:\", subtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOIs:   0%|          | 0/233 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-rc4: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-ac2: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-rc1: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-rc2: list index out of range\n",
      "Unexpected structure in response for DOI 10.47688/rba_archives_2006/18711: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-ac5: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-ac1: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-ac4: list index out of range\n",
      "Unexpected structure in response for DOI 10.3403/02855532: list index out of range\n",
      "Unexpected structure in response for DOI 10.5194/angeo-2018-95-ac3: list index out of range\n",
      "Unexpected structure in response for DOI 10.3403/bsen60838-2: list index out of range\n",
      "Unexpected structure in response for DOI 10.3403/bsen60838: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOIs:   1%|▏         | 3/233 [00:06<05:27,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected structure in response for DOI 10.4000/interfaces: list index out of range\n",
      "Unexpected structure in response for DOI 10.4000/interfaces.3227: list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOIs:   3%|▎         | 7/233 [00:17<08:34,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected structure in response for DOI 10.19064/2015.119: list index out of range\n",
      "Unexpected structure in response for DOI 10.13188/2327-204x.1000004: list index out of range\n",
      "Unexpected structure in response for DOI 10.3403/00064522u: list index out of range\n",
      "Unexpected structure in response for DOI 10.3403/00064522: list index out of range\n",
      "Unexpected structure in response for DOI 10.31579/2693-7247/030: list index out of range\n",
      "Unexpected structure in response for DOI 10.25107/2573-6051: list index out of range\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m data_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m doi \u001b[39min\u001b[39;00m subject_dois:\n\u001b[1;32m--> 105\u001b[0m     details \u001b[39m=\u001b[39m fetch_details(doi, doi_cache)\n\u001b[0;32m    106\u001b[0m     data_list\u001b[39m.\u001b[39mextend(details)\n\u001b[0;32m    108\u001b[0m \u001b[39m# Convert list of dictionaries to DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 12\u001b[0m, in \u001b[0;36mfetch_details\u001b[1;34m(doi, cache)\u001b[0m\n\u001b[0;32m     10\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://api.crossref.org/works/\u001b[39m\u001b[39m{\u001b[39;00mdoi\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     13\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n\u001b[0;32m     14\u001b[0m     data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        url = f\"https://api.crossref.org/works/{doi}\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            message = data.get('message', {})\n",
    "            \n",
    "            # Extract common details\n",
    "            title = message.get('title', [None])[0]\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])[0]\n",
    "            field = message.get('subject', [None])[0]\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "            \n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal,\n",
    "                'Field': field,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "            \n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "        except (KeyError, IndexError) as e:\n",
    "            print(f\"Unexpected structure in response for DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "# Rest of your code (Step 1, Step 2, and the progress bar)...\n",
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "\n",
    "# Initialize a DataFrame to store DOIs\n",
    "doi_df = pd.DataFrame(columns=['DOI'])\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(subjects_list), desc=\"Fetching DOIs\")\n",
    "\n",
    "# Step 2: Iterate through each subject title and fetch 25 unique DOIs\n",
    "for subject_name in subjects_list:\n",
    "    doi_url = f\"https://api.crossref.org/works?query=subject:{subject_name}&rows=25\"\n",
    "    response = requests.get(doi_url)\n",
    "    data = response.json()\n",
    "    subject_dois = [item['DOI'] for item in data.get('message', {}).get('items', [])]\n",
    "    \n",
    "    # Add the fetched DOIs to the doi_df DataFrame\n",
    "    temp_doi_df = pd.DataFrame(subject_dois, columns=['DOI'])\n",
    "    doi_df = pd.concat([doi_df, temp_doi_df], ignore_index=True)\n",
    "    \n",
    "    # Fetch details for each DOI and populate the full_df DataFrame\n",
    "    data_list = []\n",
    "    for doi in subject_dois:\n",
    "        details = fetch_details(doi, doi_cache)\n",
    "        data_list.extend(details)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    new_data_df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Update the progress bar\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Break the loop if we have fetched details for all DOIs\n",
    "    if len(doi_df) >= 25 * len(subjects_list):\n",
    "        break\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Display the resulting full_df DataFrame with DOI details\n",
    "print(new_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AHematology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3APhysiology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Environmental+Science\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Medicine\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Business%2C+Management+and+Accounting\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Chemistry\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AReligious+studies\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3ACatalysis\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AOncology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3ABiochemistry\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AEnergy+Engineering+and+Power+Technology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AOrganic+Chemistry\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AStrategy+and+Management\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3ACancer+Research\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AHorticulture\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AIndustrial+relations\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Engineering\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AImmunology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3ACell+Biology\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AGeneral+Earth+and+Planetary+Sciences\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3APhysiology+%28medical%29\n",
      "Error querying Crossref: 400 Client Error: Bad Request for url: https://api.crossref.org/works?filter=subject%3AFuel+Technology\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOI Details: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from crossref.restful import Works\n",
    "from crossref.restful import Journals\n",
    "from habanero import Crossref\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        try:\n",
    "            work = Journals().works(ids=doi)\n",
    "            message = work['message']\n",
    "            \n",
    "            # Extract common details\n",
    "            title = message.get('title', [None])[0]\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])[0]\n",
    "            field = message.get('subject', [None])[0]\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "            \n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal,\n",
    "                'Field': field,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "            \n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "# Rest of your code (Step 1, Step 2, and the progress bar)...\n",
    "\n",
    "def fetch_dois(subject_list, cr):\n",
    "    doi_list = []\n",
    "    for subject_name in subject_list:\n",
    "        try:\n",
    "            items = cr.works(query=f'subject:{subject_name}', rows=25)\n",
    "            subject_dois = [item['DOI'] for item in items['message']['items']]\n",
    "        except HTTPError as e:\n",
    "            print(f\"Error querying Crossref: {e}\")\n",
    "            subject_dois = []\n",
    "\n",
    "        doi_list.extend(subject_dois)\n",
    "    \n",
    "        if len(doi_list) >= 25 * len(subject_list):\n",
    "            break\n",
    "    \n",
    "    return doi_list\n",
    "\n",
    "\n",
    "def fetch_dois(subject_list, cr):\n",
    "    doi_list = []\n",
    "    for subject_name in subject_list:\n",
    "        try:\n",
    "            items = cr.works(filter={'subject': subject_name}, rows=25)\n",
    "            subject_dois = [item['DOI'] for item in items['message']['items']]\n",
    "        except HTTPError as e:\n",
    "            print(f\"Error querying Crossref: {e}\")\n",
    "            subject_dois = []\n",
    "\n",
    "        doi_list.extend(subject_dois)\n",
    "    \n",
    "        if len(doi_list) >= 25 * len(subject_list):\n",
    "            break\n",
    "    \n",
    "    return doi_list\n",
    "\n",
    "# Rest of your code (Step 1, Step 2, and the progress bar)...\n",
    "\n",
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "cr = Crossref()\n",
    "\n",
    "try:\n",
    "    items = cr.works(filter={'has-abstract': True}, rows=1000)\n",
    "    items = items['message']['items']\n",
    "except HTTPError as e:\n",
    "    print(f\"Error querying Crossref: {e}\")\n",
    "    items = []\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in items:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "\n",
    "# Fetch DOIs for each subject\n",
    "doi_list = fetch_dois(subjects_list, cr)\n",
    "\n",
    "# Initialize a DataFrame to store DOIs\n",
    "doi_df = pd.DataFrame({'DOI': doi_list})\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(doi_list), desc=\"Fetching DOI Details\")\n",
    "\n",
    "# Fetch details for each DOI and populate the new_data_df DataFrame\n",
    "data_list = []\n",
    "for doi in doi_list:\n",
    "    details = fetch_details(doi, doi_cache)\n",
    "    data_list.extend(details)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Display the resulting new_data_df DataFrame with DOI details\n",
    "print(new_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'details_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m details_list\u001b[39m.\u001b[39mdescribe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'details_list' is not defined"
     ]
    }
   ],
   "source": [
    "details_list.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOIs: 100%|██████████| 233/233 [01:30<00:00,  2.57it/s]\n",
      "Fetching DOI Details:   0%|          | 3/4325 [00:00<14:52,  4.84it/s]"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m data_list \u001b[39m=\u001b[39m []\n\u001b[0;32m    106\u001b[0m \u001b[39mfor\u001b[39;00m doi \u001b[39min\u001b[39;00m doi_list:\n\u001b[1;32m--> 107\u001b[0m     details \u001b[39m=\u001b[39m fetch_details(doi, doi_cache)\n\u001b[0;32m    108\u001b[0m     data_list\u001b[39m.\u001b[39mextend(details)\n\u001b[0;32m    109\u001b[0m     pbar\u001b[39m.\u001b[39mupdate(\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m, in \u001b[0;36mfetch_details\u001b[1;34m(doi, cache)\u001b[0m\n\u001b[0;32m     18\u001b[0m title \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mNone\u001b[39;00m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m abstract \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m---> 20\u001b[0m journal \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mcontainer-title\u001b[39;49m\u001b[39m'\u001b[39;49m, [\u001b[39mNone\u001b[39;49;00m])[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     21\u001b[0m field \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39msubject\u001b[39m\u001b[39m'\u001b[39m, [\u001b[39mNone\u001b[39;00m])[\u001b[39m0\u001b[39m]\n\u001b[0;32m     22\u001b[0m citation_count \u001b[39m=\u001b[39m message\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mis-referenced-by-count\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        try:\n",
    "            response = requests.get(f\"https://api.crossref.org/works/{doi}\")\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            message = data['message']\n",
    "            \n",
    "            # Extract common details\n",
    "            title = message.get('title', [None])[0]\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])\n",
    "            field = message.get('subject', [None])\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "            \n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal[0] if journal else None,\n",
    "                'Field': field[0] if field else None,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "            \n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "            \n",
    "        except HTTPError as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "\n",
    "# Rest of your code (Step 1, Step 2, and the progress bar)...\n",
    "\n",
    "# Step 1: Query CrossRef for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract items from the returned data\n",
    "items = data.get('message', {}).get('items', [])\n",
    "\n",
    "# Extract subjects from the returned items and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in items:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)\n",
    "\n",
    "# Fetch DOIs for each subject\n",
    "doi_list = []\n",
    "for subject_name in tqdm(subjects_list, desc=\"Fetching DOIs\"):\n",
    "    try:\n",
    "        doi_url = f\"https://api.crossref.org/works?query=subject:{subject_name}&rows=25\"\n",
    "        response = requests.get(doi_url)\n",
    "        data = response.json()\n",
    "        subject_dois = [item['DOI'] for item in data.get('message', {}).get('items', [])]\n",
    "        doi_list.extend(subject_dois)\n",
    "    except HTTPError as e:\n",
    "        print(f\"Error querying Crossref for subject {subject_name}: {e}\")\n",
    "\n",
    "# Initialize a DataFrame to store DOIs\n",
    "doi_df = pd.DataFrame({'DOI': doi_list})\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(doi_list), desc=\"Fetching DOI Details\")\n",
    "\n",
    "# Fetch details for each DOI and populate the new_data_df DataFrame\n",
    "data_list = []\n",
    "for doi in doi_list:\n",
    "    details = fetch_details(doi, doi_cache)\n",
    "    data_list.extend(details)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Display the resulting new_data_df DataFrame with DOI details\n",
    "print(new_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Query Crossref for a broad set of results (you can refine this as needed)\n",
    "URL = \"https://api.crossref.org/works?rows=1000\"\n",
    "response = requests.get(URL)\n",
    "data = response.json()\n",
    "\n",
    "# Extract subjects from the returned works and add to a set for uniqueness\n",
    "subjects_set = set()\n",
    "for item in data['message']['items']:\n",
    "    if 'subject' in item:\n",
    "        subjects_set.update(item['subject'])\n",
    "\n",
    "# Convert the set to a list\n",
    "subjects_list = list(subjects_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nursing (miscellaneous)',\n",
       " 'Hematology',\n",
       " 'Gerontology',\n",
       " 'Surfaces and Interfaces',\n",
       " 'Toxicology',\n",
       " 'Health, Toxicology and Mutagenesis',\n",
       " 'Pollution',\n",
       " 'General Pharmacology, Toxicology and Pharmaceutics',\n",
       " 'Leadership and Management',\n",
       " 'Business and International Management',\n",
       " 'Nutrition and Dietetics',\n",
       " 'Soil Science',\n",
       " 'Plant Science',\n",
       " 'Applied Mathematics',\n",
       " 'Dermatology',\n",
       " 'Radiation',\n",
       " 'Chemical Engineering (miscellaneous)',\n",
       " 'Pulmonary and Respiratory Medicine',\n",
       " 'Mechanics of Materials',\n",
       " 'Statistics and Probability',\n",
       " 'General Business, Management and Accounting',\n",
       " 'Colloid and Surface Chemistry',\n",
       " 'Space and Planetary Science',\n",
       " 'Geophysics',\n",
       " 'General Physics and Astronomy',\n",
       " 'Aerospace Engineering',\n",
       " 'General Social Sciences',\n",
       " 'Molecular Medicine',\n",
       " 'Law',\n",
       " 'History and Philosophy of Science',\n",
       " 'Nuclear and High Energy Physics',\n",
       " 'Catalysis',\n",
       " 'Algebra and Number Theory',\n",
       " 'Pharmacology',\n",
       " 'Spectroscopy',\n",
       " 'Multidisciplinary',\n",
       " 'Education',\n",
       " 'Social Sciences (miscellaneous)',\n",
       " 'Atmospheric Science',\n",
       " 'Pathology and Forensic Medicine',\n",
       " 'Theoretical Computer Science',\n",
       " 'Global and Planetary Change',\n",
       " 'Numerical Analysis',\n",
       " 'Orthopedics and Sports Medicine',\n",
       " 'Geochemistry and Petrology',\n",
       " 'Ocean Engineering',\n",
       " 'Information Systems',\n",
       " 'Metals and Alloys',\n",
       " 'Epidemiology',\n",
       " 'Immunology and Allergy',\n",
       " 'Political Science and International Relations',\n",
       " 'Pharmaceutical Science',\n",
       " 'Family Practice',\n",
       " 'Sociology and Political Science',\n",
       " 'General Materials Science',\n",
       " 'Control and Systems Engineering',\n",
       " 'Aquatic Science',\n",
       " 'Anatomy',\n",
       " 'Developmental and Educational Psychology',\n",
       " 'Management Science and Operations Research',\n",
       " 'Biotechnology',\n",
       " 'Agricultural and Biological Sciences (miscellaneous)',\n",
       " 'Aging',\n",
       " 'Automotive Engineering',\n",
       " 'Physiology',\n",
       " 'Oceanography',\n",
       " 'Safety, Risk, Reliability and Quality',\n",
       " 'Development',\n",
       " 'Engineering (miscellaneous)',\n",
       " 'Biological Psychiatry',\n",
       " 'Medical–Surgical Nursing',\n",
       " 'General Arts and Humanities',\n",
       " 'Histology',\n",
       " 'Health Professions (miscellaneous)',\n",
       " 'Animal Science and Zoology',\n",
       " 'Electrochemistry',\n",
       " 'Waste Management and Disposal',\n",
       " 'Industrial and Manufacturing Engineering',\n",
       " 'General Chemistry',\n",
       " 'Nephrology',\n",
       " 'Biochemistry (medical)',\n",
       " 'General Immunology and Microbiology',\n",
       " 'Archeology',\n",
       " 'Public Health, Environmental and Occupational Health',\n",
       " 'Community and Home Care',\n",
       " 'Cardiology and Cardiovascular Medicine',\n",
       " 'Condensed Matter Physics',\n",
       " 'Finance',\n",
       " 'Applied Microbiology and Biotechnology',\n",
       " 'Strategy and Management',\n",
       " 'Complementary and alternative medicine',\n",
       " 'Literature and Literary Theory',\n",
       " 'Infectious Diseases',\n",
       " 'Agronomy and Crop Science',\n",
       " 'Clinical Biochemistry',\n",
       " 'Pharmacology (medical)',\n",
       " 'Immunology',\n",
       " 'Cell Biology',\n",
       " 'Economics and Econometrics',\n",
       " 'Medicine (miscellaneous)',\n",
       " 'Sensory Systems',\n",
       " 'Bioengineering',\n",
       " 'General Economics, Econometrics and Finance',\n",
       " 'Signal Processing',\n",
       " 'Neurology',\n",
       " 'Embryology',\n",
       " 'Health (social science)',\n",
       " 'General Psychology',\n",
       " 'Information Systems and Management',\n",
       " 'Ceramics and Composites',\n",
       " 'Insect Science',\n",
       " 'Computer Graphics and Computer-Aided Design',\n",
       " 'Health Policy',\n",
       " 'Fuel Technology',\n",
       " 'Linguistics and Language',\n",
       " 'Biophysics',\n",
       " 'Computer Science Applications',\n",
       " 'Electrical and Electronic Engineering',\n",
       " 'Geography, Planning and Development',\n",
       " 'General Veterinary',\n",
       " 'General Computer Science',\n",
       " 'Obstetrics and Gynecology',\n",
       " 'Endocrinology',\n",
       " 'Hepatology',\n",
       " 'Management of Technology and Innovation',\n",
       " 'Surgery',\n",
       " 'Food Science',\n",
       " 'Classics',\n",
       " 'Biomaterials',\n",
       " 'Computer Networks and Communications',\n",
       " 'Developmental Biology',\n",
       " 'Drug Discovery',\n",
       " 'Religious studies',\n",
       " 'Renewable Energy, Sustainability and the Environment',\n",
       " 'Museology',\n",
       " 'Physics and Astronomy (miscellaneous)',\n",
       " 'Analysis',\n",
       " 'Analytical Chemistry',\n",
       " 'General Nursing',\n",
       " 'Applied Psychology',\n",
       " 'General Agricultural and Biological Sciences',\n",
       " 'Ecology',\n",
       " 'Philosophy',\n",
       " 'Polymers and Plastics',\n",
       " 'Earth-Surface Processes',\n",
       " 'Rehabilitation',\n",
       " 'Internal Medicine',\n",
       " 'Demography',\n",
       " 'General Neuroscience',\n",
       " 'Arts and Humanities (miscellaneous)',\n",
       " 'Otorhinolaryngology',\n",
       " 'General Dentistry',\n",
       " 'Visual Arts and Performing Arts',\n",
       " 'Molecular Biology',\n",
       " 'Horticulture',\n",
       " 'Clinical Psychology',\n",
       " 'Geriatrics and Gerontology',\n",
       " 'General Engineering',\n",
       " 'Language and Linguistics',\n",
       " 'Environmental Chemistry',\n",
       " 'Neurology (clinical)',\n",
       " 'Electronic, Optical and Magnetic Materials',\n",
       " 'Organizational Behavior and Human Resource Management',\n",
       " 'Management Information Systems',\n",
       " 'Library and Information Sciences',\n",
       " 'Chemistry (miscellaneous)',\n",
       " 'Radiology, Nuclear Medicine and imaging',\n",
       " 'Psychiatry and Mental health',\n",
       " 'Behavioral Neuroscience',\n",
       " 'Statistical and Nonlinear Physics',\n",
       " 'Geology',\n",
       " 'Urology',\n",
       " 'Discrete Mathematics and Combinatorics',\n",
       " 'Pediatrics, Perinatology and Child Health',\n",
       " 'Nuclear Energy and Engineering',\n",
       " 'General Mathematics',\n",
       " 'Cellular and Molecular Neuroscience',\n",
       " 'Physical and Theoretical Chemistry',\n",
       " 'Atomic and Molecular Physics, and Optics',\n",
       " 'Parasitology',\n",
       " 'Rheumatology',\n",
       " 'Experimental and Cognitive Psychology',\n",
       " 'Management, Monitoring, Policy and Law',\n",
       " 'General Chemical Engineering',\n",
       " 'Surfaces, Coatings and Films',\n",
       " 'General Environmental Science',\n",
       " 'Astronomy and Astrophysics',\n",
       " 'Mechanical Engineering',\n",
       " 'Materials Chemistry',\n",
       " 'General Medicine',\n",
       " 'Media Technology',\n",
       " 'Music',\n",
       " 'General Biochemistry, Genetics and Molecular Biology',\n",
       " 'Physical Therapy, Sports Therapy and Rehabilitation',\n",
       " 'Software',\n",
       " 'Anthropology',\n",
       " 'Computational Theory and Mathematics',\n",
       " 'Oral Surgery',\n",
       " 'Hardware and Architecture',\n",
       " 'Artificial Intelligence',\n",
       " 'Biomedical Engineering',\n",
       " 'Water Science and Technology',\n",
       " 'Oncology',\n",
       " 'Biochemistry',\n",
       " 'Microbiology (medical)',\n",
       " 'Energy Engineering and Power Technology',\n",
       " 'Critical Care and Intensive Care Medicine',\n",
       " 'Issues, ethics and legal aspects',\n",
       " 'Genetics',\n",
       " 'Gastroenterology',\n",
       " 'Organic Chemistry',\n",
       " 'Cultural Studies',\n",
       " 'Ecology, Evolution, Behavior and Systematics',\n",
       " 'Cancer Research',\n",
       " 'Industrial relations',\n",
       " 'Materials Science (miscellaneous)',\n",
       " 'Speech and Hearing',\n",
       " 'Building and Construction',\n",
       " 'Endocrinology, Diabetes and Metabolism',\n",
       " 'Anesthesiology and Pain Medicine',\n",
       " 'Instrumentation',\n",
       " 'General Earth and Planetary Sciences',\n",
       " 'Health Informatics',\n",
       " 'Environmental Engineering',\n",
       " 'Physiology (medical)',\n",
       " 'Virology',\n",
       " 'Marketing',\n",
       " 'Earth and Planetary Sciences (miscellaneous)',\n",
       " 'Genetics (clinical)',\n",
       " 'History',\n",
       " 'Modeling and Simulation',\n",
       " 'Ophthalmology',\n",
       " 'Communication']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching DOIs:   0%|          | 0/2 [00:54<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UrlSyntaxError",
     "evalue": "Field query subject specified but there is no such field query for this route. Valid field queries for this route are: affiliation, author, bibliographic, chair, container_title, contributor, editor, event_acronym, event_location, event_name, event_sponsor, event_theme, funder_name, publisher_location, publisher_name, translator",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUrlSyntaxError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m subject_name \u001b[39min\u001b[39;00m subject_list:\n\u001b[0;32m     75\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 76\u001b[0m         items \u001b[39m=\u001b[39m Works()\u001b[39m.\u001b[39;49mquery(subject\u001b[39m=\u001b[39;49msubject_name, rows\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n\u001b[0;32m     77\u001b[0m         subject_dois \u001b[39m=\u001b[39m [item[\u001b[39m'\u001b[39m\u001b[39mDOI\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m items[\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mitems\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m     78\u001b[0m     \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\crossref\\restful.py:933\u001b[0m, in \u001b[0;36mWorks.query\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[39mif\u001b[39;00m field \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFIELDS_QUERY:\n\u001b[0;32m    928\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    929\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mField query \u001b[39m\u001b[39m{\u001b[39;00mfield\u001b[39m!s}\u001b[39;00m\u001b[39m specified but there is no such field query for\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    930\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m this route.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    931\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Valid field queries for this route are: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mFIELDS_QUERY)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    932\u001b[0m         )\n\u001b[1;32m--> 933\u001b[0m         \u001b[39mraise\u001b[39;00m UrlSyntaxError(\n\u001b[0;32m    934\u001b[0m             msg,\n\u001b[0;32m    935\u001b[0m         )\n\u001b[0;32m    936\u001b[0m     request_params[\u001b[39m\"\u001b[39m\u001b[39mquery.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m field\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m)] \u001b[39m=\u001b[39m value\n\u001b[0;32m    938\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m(request_url\u001b[39m=\u001b[39mrequest_url,\n\u001b[0;32m    939\u001b[0m                       request_params\u001b[39m=\u001b[39mrequest_params,\n\u001b[0;32m    940\u001b[0m                       context\u001b[39m=\u001b[39mcontext,\n\u001b[0;32m    941\u001b[0m                       etiquette\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39metiquette,\n\u001b[0;32m    942\u001b[0m                       timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout)\n",
      "\u001b[1;31mUrlSyntaxError\u001b[0m: Field query subject specified but there is no such field query for this route. Valid field queries for this route are: affiliation, author, bibliographic, chair, container_title, contributor, editor, event_acronym, event_location, event_name, event_sponsor, event_theme, funder_name, publisher_location, publisher_name, translator"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from crossref.restful import Works\n",
    "from habanero import Crossref\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "def fetch_details(doi, cache):\n",
    "    if doi in cache:\n",
    "        common_details = cache[doi]['common']\n",
    "        authors = cache[doi]['authors']\n",
    "    else:\n",
    "        try:\n",
    "            work = Works().doi(doi)\n",
    "            message = work['message']\n",
    "            \n",
    "            # Extract common details\n",
    "            title = message.get('title', [None])[0]\n",
    "            abstract = message.get('abstract', None)\n",
    "            journal = message.get('container-title', [None])[0]\n",
    "            field = message.get('subject', [None])[0]\n",
    "            citation_count = message.get('is-referenced-by-count', None)\n",
    "            date_received = message.get('created', {}).get('date-time', None)\n",
    "            date_published = message.get('published-online', {}).get('date-parts', [None])[0]\n",
    "            address = message.get('publisher-location', None)\n",
    "            language = message.get('language', None)\n",
    "            \n",
    "            common_details = {\n",
    "                'DOI': doi,\n",
    "                'Title': title,\n",
    "                'Abstract': abstract,\n",
    "                'Journal': journal,\n",
    "                'Field': field,\n",
    "                'Citation Count': citation_count,\n",
    "                'Date Received': date_received,\n",
    "                'Date Published': date_published,\n",
    "                'Address': address,\n",
    "                'Language': language\n",
    "            }\n",
    "            \n",
    "            authors = message.get('author', [])\n",
    "            cache[doi] = {'common': common_details, 'authors': authors}\n",
    "            \n",
    "        except HTTPError as e:\n",
    "            print(f\"Error fetching DOI {doi}: {e}\")\n",
    "            return []\n",
    "\n",
    "    # Getting authors details\n",
    "    details_list = []\n",
    "    for idx, author in enumerate(authors):\n",
    "        author_first_name = author.get('given', None)\n",
    "        author_last_name = author.get('family', None)\n",
    "        author_order = idx + 1\n",
    "\n",
    "        details = common_details.copy()\n",
    "        details['Author First Name'] = author_first_name\n",
    "        details['Author Last Name'] = author_last_name\n",
    "        details['Author Order'] = author_order\n",
    "\n",
    "        details_list.append(details)\n",
    "\n",
    "    return details_list\n",
    "\n",
    "# Step 1: Specify the subject(s) you want to search for\n",
    "subject_list = ['Immunology', 'Cell Biology']\n",
    "\n",
    "# Initialize a cache for storing details of already processed DOIs\n",
    "doi_cache = {}\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(subject_list), desc=\"Fetching DOIs\")\n",
    "\n",
    "# Fetch DOIs for each subject\n",
    "doi_list = []\n",
    "for subject_name in subject_list:\n",
    "    try:\n",
    "        items = Works().query(subject=subject_name, rows=25)\n",
    "        subject_dois = [item['DOI'] for item in items['message']['items']]\n",
    "    except HTTPError as e:\n",
    "        print(f\"Error querying Crossref: {e}\")\n",
    "        subject_dois = []\n",
    "\n",
    "    doi_list.extend(subject_dois)\n",
    "    \n",
    "    pbar.update(1)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Initialize a DataFrame to store DOIs\n",
    "doi_df = pd.DataFrame({'DOI': doi_list})\n",
    "\n",
    "# Initialize tqdm for the progress bar\n",
    "pbar = tqdm(total=len(doi_list), desc=\"Fetching DOI Details\")\n",
    "\n",
    "# Fetch details for each DOI and populate the new_data_df DataFrame\n",
    "data_list = []\n",
    "for doi in doi_list:\n",
    "    details = fetch_details(doi, doi_cache)\n",
    "    data_list.extend(details)\n",
    "    pbar.update(1)\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "new_data_df = pd.DataFrame(data_list)\n",
    "\n",
    "# Close the progress bar\n",
    "pbar.close()\n",
    "\n",
    "# Display the resulting new_data_df DataFrame with DOI details\n",
    "print(new_data_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No articles found for the specified subject.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "\n",
    "def get_random_article(subject):\n",
    "    base_url = f'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'query.bibliographic': subject,\n",
    "        'sort': 'relevance',\n",
    "        'rows': 1,\n",
    "        'sample': 'true'\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        article = data['message']['items'][0]\n",
    "        return article\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "subject = 'Cell Biology'\n",
    "article = get_random_article(subject)\n",
    "\n",
    "if article:\n",
    "    doi = article.get('DOI', '')\n",
    "    title = article.get('title', [''])[0]\n",
    "    abstract = article.get('abstract', '')\n",
    "    journal = article.get('container-title', [''])[0]\n",
    "    field = ', '.join(article.get('subject', []))\n",
    "    citation_count = article.get('is-referenced-by-count', 0)\n",
    "    date_received = article.get('created', '')\n",
    "    date_published = article.get('published-print', '')\n",
    "    address = ', '.join(article.get('author', []))\n",
    "    language = article.get('language', '')\n",
    "\n",
    "    print(f\"DOI: {doi}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Abstract: {abstract}\")\n",
    "    print(f\"Journal: {journal}\")\n",
    "    print(f\"Field: {field}\")\n",
    "    print(f\"Citation Count: {citation_count}\")\n",
    "    print(f\"Date Received: {date_received}\")\n",
    "    print(f\"Date Published: {date_published}\")\n",
    "    print(f\"Address: {address}\")\n",
    "    print(f\"Language: {language}\")\n",
    "else:\n",
    "    print(\"No articles found for the specified subject.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m progress_bar \u001b[39m=\u001b[39m tqdm(total\u001b[39m=\u001b[39mtotal_articles, desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFetching Articles\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(articles) \u001b[39m<\u001b[39m total_articles:\n\u001b[1;32m---> 30\u001b[0m     doi \u001b[39m=\u001b[39m get_random_doi()\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m doi:\n\u001b[0;32m     32\u001b[0m         article \u001b[39m=\u001b[39m get_article_info(doi)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mget_random_doi\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_random_doi\u001b[39m():\n\u001b[0;32m      7\u001b[0m     base_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://api.crossref.org/works/random\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 8\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(base_url)\n\u001b[0;32m      9\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     10\u001b[0m         data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:403\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_conn(conn)\n\u001b[0;32m    404\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    405\u001b[0m     \u001b[39m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mconn\u001b[39m.\u001b[39mtimeout)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:1053\u001b[0m, in \u001b[0;36mHTTPSConnectionPool._validate_conn\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m   1051\u001b[0m \u001b[39m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(conn, \u001b[39m\"\u001b[39m\u001b[39msock\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):  \u001b[39m# AppEngine might not have  `.sock`\u001b[39;00m\n\u001b[1;32m-> 1053\u001b[0m     conn\u001b[39m.\u001b[39;49mconnect()\n\u001b[0;32m   1055\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m conn\u001b[39m.\u001b[39mis_verified:\n\u001b[0;32m   1056\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1057\u001b[0m         (\n\u001b[0;32m   1058\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnverified HTTPS request is being made to host \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1063\u001b[0m         InsecureRequestWarning,\n\u001b[0;32m   1064\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:363\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Add certificate verification\u001b[39;00m\n\u001b[1;32m--> 363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msock \u001b[39m=\u001b[39m conn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_new_conn()\n\u001b[0;32m    364\u001b[0m     hostname \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost\n\u001b[0;32m    365\u001b[0m     tls_in_tls \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connection.py:174\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    171\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39msocket_options\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket_options\n\u001b[0;32m    173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     conn \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mcreate_connection(\n\u001b[0;32m    175\u001b[0m         (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dns_host, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mport), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[39mexcept\u001b[39;00m SocketTimeout:\n\u001b[0;32m    179\u001b[0m     \u001b[39mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[0;32m    180\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m    181\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mConnection to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m timed out. (connect timeout=\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    182\u001b[0m         \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhost, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout),\n\u001b[0;32m    183\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\util\\connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[39mif\u001b[39;00m source_address:\n\u001b[0;32m     84\u001b[0m         sock\u001b[39m.\u001b[39mbind(source_address)\n\u001b[1;32m---> 85\u001b[0m     sock\u001b[39m.\u001b[39mconnect(sa)\n\u001b[0;32m     86\u001b[0m     \u001b[39mreturn\u001b[39;00m sock\n\u001b[0;32m     88\u001b[0m \u001b[39mexcept\u001b[39;00m socket\u001b[39m.\u001b[39merror \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Make sure to install the tqdm library\n",
    "\n",
    "def get_random_doi():\n",
    "    base_url = f'https://api.crossref.org/works/random'\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        doi = data['message']['DOI']\n",
    "        return doi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_article_info(doi):\n",
    "    base_url = f'https://api.crossref.org/works/{doi}'\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['message']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "total_articles = 2000\n",
    "articles = []\n",
    "progress_bar = tqdm(total=total_articles, desc='Fetching Articles')\n",
    "\n",
    "while len(articles) < total_articles:\n",
    "    doi = get_random_doi()\n",
    "    if doi:\n",
    "        article = get_article_info(doi)\n",
    "        if article:\n",
    "            articles.append(article)\n",
    "            progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Abstract': [],\n",
    "    'Journal': [],\n",
    "    'Field': [],\n",
    "    'Citation Count': [],\n",
    "    'Date Received': [],\n",
    "    'Date Published': [],\n",
    "    'Address': [],\n",
    "    'Language': []\n",
    "}\n",
    "\n",
    "for article in articles:\n",
    "    data['DOI'].append(article.get('DOI', ''))\n",
    "    data['Title'].append(article.get('title', [''])[0])\n",
    "    data['Abstract'].append(article.get('abstract', ''))\n",
    "    data['Journal'].append(article.get('container-title', [''])[0])\n",
    "    data['Field'].append(', '.join(article.get('subject', [])))\n",
    "    data['Citation Count'].append(article.get('is-referenced-by-count', 0))\n",
    "    data['Date Received'].append(article.get('created', ''))\n",
    "    data['Date Published'].append(article.get('published-print', ''))\n",
    "    data['Address'].append(', '.join(article.get('author', [])))\n",
    "    data['Language'].append(article.get('language', ''))\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Obtaining DOIs:   0%|          | 0/2000 [18:41<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 34\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mlen\u001b[39m(articles) \u001b[39m<\u001b[39m total_articles:\n\u001b[0;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         doi \u001b[39m=\u001b[39m get_random_doi()\n\u001b[0;32m     35\u001b[0m         \u001b[39mif\u001b[39;00m doi:\n\u001b[0;32m     36\u001b[0m             articles\u001b[39m.\u001b[39mappend(doi)\n",
      "Cell \u001b[1;32mIn[44], line 9\u001b[0m, in \u001b[0;36mget_random_doi\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_random_doi\u001b[39m():\n\u001b[0;32m      8\u001b[0m     base_url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://api.crossref.org/works/random\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(base_url)\n\u001b[0;32m     10\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m     11\u001b[0m         data \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[0;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    715\u001b[0m     conn,\n\u001b[0;32m    716\u001b[0m     method,\n\u001b[0;32m    717\u001b[0m     url,\n\u001b[0;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    722\u001b[0m )\n\u001b[0;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[0;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[0;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[0;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[0;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[0;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\urllib3\\connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[0;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[0;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[0;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[0;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[0;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[1;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[0;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[0;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm  # Make sure to install the tqdm library\n",
    "\n",
    "def get_random_doi():\n",
    "    base_url = f'https://api.crossref.org/works/random'\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        doi = data['message']['DOI']\n",
    "        return doi\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_article_info(doi):\n",
    "    base_url = f'https://api.crossref.org/works/{doi}'\n",
    "    response = requests.get(base_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['message']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "total_articles = 2000\n",
    "articles = []\n",
    "doi_progress_bar = tqdm(total=total_articles, desc='Obtaining DOIs')\n",
    "\n",
    "timeout = 10  # Set a timeout value in seconds\n",
    "\n",
    "while len(articles) < total_articles:\n",
    "    try:\n",
    "        doi = get_random_doi()\n",
    "        if doi:\n",
    "            articles.append(doi)\n",
    "            doi_progress_bar.update(1)\n",
    "    except (requests.exceptions.Timeout, requests.exceptions.RequestException):\n",
    "        pass  # Ignore timeout and other request exceptions, and continue\n",
    "\n",
    "doi_progress_bar.close()\n",
    "\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Abstract': [],\n",
    "    'Journal': [],\n",
    "    'Field': [],\n",
    "    'Citation Count': [],\n",
    "    'Date Received': [],\n",
    "    'Date Published': [],\n",
    "    'Address': [],\n",
    "    'Language': []\n",
    "}\n",
    "\n",
    "info_progress_bar = tqdm(articles, desc='Fetching Article Info')\n",
    "\n",
    "requests_per_second = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for doi in info_progress_bar:\n",
    "    if requests_per_second >= 50:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        if elapsed_time < 1:\n",
    "            time.sleep(1 - elapsed_time)\n",
    "        requests_per_second = 0\n",
    "        start_time = time.time()\n",
    "\n",
    "    article = get_article_info(doi)\n",
    "    if article:\n",
    "        data['DOI'].append(doi)\n",
    "        data['Title'].append(article.get('title', [''])[0])\n",
    "        data['Abstract'].append(article.get('abstract', ''))\n",
    "        data['Journal'].append(article.get('container-title', [''])[0])\n",
    "        data['Field'].append(', '.join(article.get('subject', [])))\n",
    "        data['Citation Count'].append(article.get('is-referenced-by-count', 0))\n",
    "        data['Date Received'].append(article.get('created', ''))\n",
    "        data['Date Published'].append(article.get('published-print', ''))\n",
    "        data['Address'].append(', '.join(article.get('author', [])))\n",
    "        data['Language'].append(article.get('language', ''))\n",
    "\n",
    "        requests_per_second += 1\n",
    "\n",
    "info_progress_bar.close()\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indexed: {'date-parts': [[2022, 4, 3]], 'date-time': '2022-04-03T01:46:03Z', 'timestamp': 1648950363641}\n",
      "reference-count: 0\n",
      "publisher: Book Publisher International (a part of SCIENCEDOMAIN International)\n",
      "content-domain: {'domain': [], 'crossmark-restriction': False}\n",
      "published-print: {'date-parts': [[2021, 7, 30]]}\n",
      "DOI: 10.9734/bpi/hmms/v13/2889f\n",
      "type: book-chapter\n",
      "created: {'date-parts': [[2021, 8, 6]], 'date-time': '2021-08-06T10:42:23Z', 'timestamp': 1628246543000}\n",
      "page: 108-114\n",
      "source: Crossref\n",
      "is-referenced-by-count: 0\n",
      "title:\n",
      "  - A Review on MVD for Trigeminal Neuralgia\n",
      "prefix: 10.9734\n",
      "author:\n",
      "  - {'given': 'Renuka S.', 'family': 'Melkundi', 'sequence': 'first', 'affiliation': []}\n",
      "  - {'given': 'Sateesh', 'family': 'Melkundi', 'sequence': 'additional', 'affiliation': []}\n",
      "member: 4694\n",
      "published-online: {'date-parts': [[2021, 7, 30]]}\n",
      "container-title:\n",
      "  - Highlights on Medicine and Medical Science Vol. 13\n",
      "deposited: {'date-parts': [[2021, 8, 6]], 'date-time': '2021-08-06T10:42:35Z', 'timestamp': 1628246555000}\n",
      "score: 0.0\n",
      "resource: {'primary': {'URL': 'https://stm.bookpi.org/HMMS-V13/article/view/2729'}}\n",
      "issued: {'date-parts': [[2021, 7, 30]]}\n",
      "references-count: 0\n",
      "URL: http://dx.doi.org/10.9734/bpi/hmms/v13/2889f\n",
      "published: {'date-parts': [[2021, 7, 30]]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_articles_by_year(year):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': 1\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data.get('message', {}).get('items', [])\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(response.content)\n",
    "        return []\n",
    "\n",
    "year_to_test = 2021\n",
    "articles = get_articles_by_year(year_to_test)\n",
    "\n",
    "if articles:\n",
    "    article = articles[0]\n",
    "    for key, value in article.items():\n",
    "        if isinstance(value, list):\n",
    "            print(f\"{key}:\")\n",
    "            for item in value:\n",
    "                print(f\"  - {item}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")\n",
    "else:\n",
    "    print(f\"No articles found for the year {year_to_test}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11347, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Container Title</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Author First Name</th>\n",
       "      <th>Author Last Name</th>\n",
       "      <th>Author Order</th>\n",
       "      <th>Referenced By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.35366/99957</td>\n",
       "      <td>Qué debe llevar un resumen</td>\n",
       "      <td>Cirujano General</td>\n",
       "      <td>GRAPHIMEDIC SA DE CV</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Abilene Cirenia</td>\n",
       "      <td>Escamilla Ortiz</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Jessé Rafael Bento</td>\n",
       "      <td>Lima</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Clayton dos Santos</td>\n",
       "      <td>Silva</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Romário Guimarães Verçosa</td>\n",
       "      <td>Araújo</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Jonas Olimpio de Lima</td>\n",
       "      <td>Silva</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Arlla Katherine Xavier</td>\n",
       "      <td>Lima</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>João Manoel</td>\n",
       "      <td>Silva</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Tania Marta Carvalho</td>\n",
       "      <td>Santos</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.34117/bjdv6n3-358</td>\n",
       "      <td>Perfil socioeconômico de mulheres feirantes do...</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>Brazilian Journal of Development</td>\n",
       "      <td>[2020]</td>\n",
       "      <td>Jakes Halan de Queiroz</td>\n",
       "      <td>Costa</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Jing</td>\n",
       "      <td>Gu</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Qiaotong</td>\n",
       "      <td>Pang</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Jinzhi</td>\n",
       "      <td>Ding</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Runsheng</td>\n",
       "      <td>Yin</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yuanhe</td>\n",
       "      <td>Yang</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.5194/egusphere-egu2020-1598</td>\n",
       "      <td>The storage and influencing factors of mercury...</td>\n",
       "      <td></td>\n",
       "      <td>Copernicus GmbH</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yanxu</td>\n",
       "      <td>Zhang</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.47240/revistadaesg.v34i72.1118</td>\n",
       "      <td>ANÁLISE CONSTRUTIVISTA DA DEMOCRATIZAÇÃO EM TA...</td>\n",
       "      <td>Revista da Escola Superior de Guerra</td>\n",
       "      <td>Revista da Escola Superior de Guerra</td>\n",
       "      <td>[]</td>\n",
       "      <td>Douglas Rocha</td>\n",
       "      <td>Almeida</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.47240/revistadaesg.v34i72.1118</td>\n",
       "      <td>ANÁLISE CONSTRUTIVISTA DA DEMOCRATIZAÇÃO EM TA...</td>\n",
       "      <td>Revista da Escola Superior de Guerra</td>\n",
       "      <td>Revista da Escola Superior de Guerra</td>\n",
       "      <td>[]</td>\n",
       "      <td>Fábio Albergaria</td>\n",
       "      <td>De Queiroz</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.37972/chgpu.2020.99.85.024</td>\n",
       "      <td>ПРИМЕНЕНИЕ РЕКРЕАЦИОННО-ОЗДОРОВИТЕЛЬНЫХ ПРАКТИ...</td>\n",
       "      <td>Bulletin of the Chuvash State Pedagogical Univ...</td>\n",
       "      <td>Yakovlev Chuvash State Pedagogical University</td>\n",
       "      <td>[]</td>\n",
       "      <td>Gulnur</td>\n",
       "      <td>Kamalova</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1101/2020.10.20.344226</td>\n",
       "      <td>The data-index: an author-level metric that va...</td>\n",
       "      <td></td>\n",
       "      <td>Cold Spring Harbor Laboratory</td>\n",
       "      <td>[]</td>\n",
       "      <td>Amelia S C</td>\n",
       "      <td>Hood</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.1101/2020.10.20.344226</td>\n",
       "      <td>The data-index: an author-level metric that va...</td>\n",
       "      <td></td>\n",
       "      <td>Cold Spring Harbor Laboratory</td>\n",
       "      <td>[]</td>\n",
       "      <td>William J</td>\n",
       "      <td>Sutherland</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  DOI  \\\n",
       "0                      10.35366/99957   \n",
       "1                10.34117/bjdv6n3-358   \n",
       "2                10.34117/bjdv6n3-358   \n",
       "3                10.34117/bjdv6n3-358   \n",
       "4                10.34117/bjdv6n3-358   \n",
       "5                10.34117/bjdv6n3-358   \n",
       "6                10.34117/bjdv6n3-358   \n",
       "7                10.34117/bjdv6n3-358   \n",
       "8                10.34117/bjdv6n3-358   \n",
       "9      10.5194/egusphere-egu2020-1598   \n",
       "10     10.5194/egusphere-egu2020-1598   \n",
       "11     10.5194/egusphere-egu2020-1598   \n",
       "12     10.5194/egusphere-egu2020-1598   \n",
       "13     10.5194/egusphere-egu2020-1598   \n",
       "14     10.5194/egusphere-egu2020-1598   \n",
       "15  10.47240/revistadaesg.v34i72.1118   \n",
       "16  10.47240/revistadaesg.v34i72.1118   \n",
       "17      10.37972/chgpu.2020.99.85.024   \n",
       "18          10.1101/2020.10.20.344226   \n",
       "19          10.1101/2020.10.20.344226   \n",
       "\n",
       "                                                Title  \\\n",
       "0                          Qué debe llevar un resumen   \n",
       "1   Perfil socioeconômico de mulheres feirantes do...   \n",
       "2   Perfil socioeconômico de mulheres feirantes do...   \n",
       "3   Perfil socioeconômico de mulheres feirantes do...   \n",
       "4   Perfil socioeconômico de mulheres feirantes do...   \n",
       "5   Perfil socioeconômico de mulheres feirantes do...   \n",
       "6   Perfil socioeconômico de mulheres feirantes do...   \n",
       "7   Perfil socioeconômico de mulheres feirantes do...   \n",
       "8   Perfil socioeconômico de mulheres feirantes do...   \n",
       "9   The storage and influencing factors of mercury...   \n",
       "10  The storage and influencing factors of mercury...   \n",
       "11  The storage and influencing factors of mercury...   \n",
       "12  The storage and influencing factors of mercury...   \n",
       "13  The storage and influencing factors of mercury...   \n",
       "14  The storage and influencing factors of mercury...   \n",
       "15  ANÁLISE CONSTRUTIVISTA DA DEMOCRATIZAÇÃO EM TA...   \n",
       "16  ANÁLISE CONSTRUTIVISTA DA DEMOCRATIZAÇÃO EM TA...   \n",
       "17  ПРИМЕНЕНИЕ РЕКРЕАЦИОННО-ОЗДОРОВИТЕЛЬНЫХ ПРАКТИ...   \n",
       "18  The data-index: an author-level metric that va...   \n",
       "19  The data-index: an author-level metric that va...   \n",
       "\n",
       "                                      Container Title  \\\n",
       "0                                    Cirujano General   \n",
       "1                    Brazilian Journal of Development   \n",
       "2                    Brazilian Journal of Development   \n",
       "3                    Brazilian Journal of Development   \n",
       "4                    Brazilian Journal of Development   \n",
       "5                    Brazilian Journal of Development   \n",
       "6                    Brazilian Journal of Development   \n",
       "7                    Brazilian Journal of Development   \n",
       "8                    Brazilian Journal of Development   \n",
       "9                                                       \n",
       "10                                                      \n",
       "11                                                      \n",
       "12                                                      \n",
       "13                                                      \n",
       "14                                                      \n",
       "15               Revista da Escola Superior de Guerra   \n",
       "16               Revista da Escola Superior de Guerra   \n",
       "17  Bulletin of the Chuvash State Pedagogical Univ...   \n",
       "18                                                      \n",
       "19                                                      \n",
       "\n",
       "                                        Publisher Publish Date  \\\n",
       "0                            GRAPHIMEDIC SA DE CV       [2020]   \n",
       "1                Brazilian Journal of Development       [2020]   \n",
       "2                Brazilian Journal of Development       [2020]   \n",
       "3                Brazilian Journal of Development       [2020]   \n",
       "4                Brazilian Journal of Development       [2020]   \n",
       "5                Brazilian Journal of Development       [2020]   \n",
       "6                Brazilian Journal of Development       [2020]   \n",
       "7                Brazilian Journal of Development       [2020]   \n",
       "8                Brazilian Journal of Development       [2020]   \n",
       "9                                 Copernicus GmbH           []   \n",
       "10                                Copernicus GmbH           []   \n",
       "11                                Copernicus GmbH           []   \n",
       "12                                Copernicus GmbH           []   \n",
       "13                                Copernicus GmbH           []   \n",
       "14                                Copernicus GmbH           []   \n",
       "15           Revista da Escola Superior de Guerra           []   \n",
       "16           Revista da Escola Superior de Guerra           []   \n",
       "17  Yakovlev Chuvash State Pedagogical University           []   \n",
       "18                  Cold Spring Harbor Laboratory           []   \n",
       "19                  Cold Spring Harbor Laboratory           []   \n",
       "\n",
       "            Author First Name Author Last Name  Author Order  Referenced By  \n",
       "0             Abilene Cirenia  Escamilla Ortiz             1              0  \n",
       "1          Jessé Rafael Bento             Lima             1              0  \n",
       "2          Clayton dos Santos            Silva             2              0  \n",
       "3   Romário Guimarães Verçosa           Araújo             3              0  \n",
       "4       Jonas Olimpio de Lima            Silva             4              0  \n",
       "5      Arlla Katherine Xavier             Lima             5              0  \n",
       "6                 João Manoel            Silva             6              0  \n",
       "7        Tania Marta Carvalho           Santos             7              0  \n",
       "8      Jakes Halan de Queiroz            Costa             8              0  \n",
       "9                        Jing               Gu             1              0  \n",
       "10                   Qiaotong             Pang             2              0  \n",
       "11                     Jinzhi             Ding             3              0  \n",
       "12                   Runsheng              Yin             4              0  \n",
       "13                     Yuanhe             Yang             5              0  \n",
       "14                      Yanxu            Zhang             6              0  \n",
       "15              Douglas Rocha          Almeida             1              0  \n",
       "16           Fábio Albergaria       De Queiroz             2              0  \n",
       "17                     Gulnur         Kamalova             1              0  \n",
       "18                 Amelia S C             Hood             1              0  \n",
       "19                  William J       Sutherland             2              0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "rows = 1000\n",
    "\n",
    "def get_articles_by_year(year, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows  # Adjust this number to change the number of articles per year\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    cursor = '*'\n",
    "\n",
    "    while cursor:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                break\n",
    "            \n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor', '')\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "years_to_pull = [2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "\n",
    "for year in years_to_pull:\n",
    "    articles = get_articles_by_year(year, rows = rows)  # Adjust this number for the desired articles per year\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "for article in all_articles:\n",
    "    doi = article.get('DOI', '')\n",
    "    title = article.get('title', [''])[0]\n",
    "    container_title = article.get('container-title', [''])[0]\n",
    "    publisher = article.get('publisher', '')\n",
    "    publish_date = article.get('published-print', {}).get('date-parts', [[]])[0]\n",
    "    authors = article.get('author', [])\n",
    "    referenced_by = article.get('is-referenced-by-count', 0)\n",
    "\n",
    "    for order, author in enumerate(authors, start=1):\n",
    "        first_name = author.get('given', '')\n",
    "        last_name = author.get('family', '')\n",
    "        \n",
    "        data['DOI'].append(doi)\n",
    "        data['Title'].append(title)\n",
    "        data['Container Title'].append(container_title)\n",
    "        data['Publisher'].append(publisher)\n",
    "        data['Publish Date'].append(publish_date)\n",
    "        data['Author First Name'].append(first_name)\n",
    "        data['Author Last Name'].append(last_name)\n",
    "        data['Author Order'].append(order)\n",
    "        data['Referenced By'].append(referenced_by)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date', 'Author First Name', 'Author Last Name', 'Author Order', 'Referenced By']\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('D:/School/GitHub/SMU_MSDS_Capstone/main.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT GOOD EXPANDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18465, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Container Title</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Author First Name</th>\n",
       "      <th>Author Last Name</th>\n",
       "      <th>Author Order</th>\n",
       "      <th>Referenced By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>L</td>\n",
       "      <td>Massaro</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>F</td>\n",
       "      <td>Ceccarelli</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>FR</td>\n",
       "      <td>Spinelli</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>F</td>\n",
       "      <td>Morello</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>C</td>\n",
       "      <td>Perricone</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>F</td>\n",
       "      <td>Miranda</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>S</td>\n",
       "      <td>Truglia</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>V</td>\n",
       "      <td>Orefice</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>IM</td>\n",
       "      <td>Rutigliano</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>C</td>\n",
       "      <td>Alessandri</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>G</td>\n",
       "      <td>Valesini</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.1136/annrheumdis-2017-eular.4238</td>\n",
       "      <td>SAT0241 Early response to belimumab in sle-rel...</td>\n",
       "      <td>Poster Presentations</td>\n",
       "      <td>BMJ Publishing Group Ltd and European League A...</td>\n",
       "      <td>[2017, 6]</td>\n",
       "      <td>F</td>\n",
       "      <td>Conti</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10.14445/22312803/ijctt-v47p113</td>\n",
       "      <td>Object Detection and Semantic Segmentation usi...</td>\n",
       "      <td>International Journal of Computer Trends and T...</td>\n",
       "      <td>Seventh Sense Research Group Journals</td>\n",
       "      <td>[]</td>\n",
       "      <td>R.Kar</td>\n",
       "      <td>thika</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.14445/22312803/ijctt-v47p113</td>\n",
       "      <td>Object Detection and Semantic Segmentation usi...</td>\n",
       "      <td>International Journal of Computer Trends and T...</td>\n",
       "      <td>Seventh Sense Research Group Journals</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10.14445/22312803/ijctt-v47p113</td>\n",
       "      <td>Object Detection and Semantic Segmentation usi...</td>\n",
       "      <td>International Journal of Computer Trends and T...</td>\n",
       "      <td>Seventh Sense Research Group Journals</td>\n",
       "      <td>[]</td>\n",
       "      <td>S.N Santha</td>\n",
       "      <td>lakshmi</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10.1080/21695717.2017.1316452</td>\n",
       "      <td>Relative role of heredity or training for musi...</td>\n",
       "      <td>Hearing, Balance and Communication</td>\n",
       "      <td>Informa UK Limited</td>\n",
       "      <td>[2017, 4, 3]</td>\n",
       "      <td>Saransh</td>\n",
       "      <td>Jain</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10.1080/21695717.2017.1316452</td>\n",
       "      <td>Relative role of heredity or training for musi...</td>\n",
       "      <td>Hearing, Balance and Communication</td>\n",
       "      <td>Informa UK Limited</td>\n",
       "      <td>[2017, 4, 3]</td>\n",
       "      <td>Harshan Kumar</td>\n",
       "      <td>H. S.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.4197/art.25-2.6</td>\n",
       "      <td>The impact of knowing the reasons of the Hadee...</td>\n",
       "      <td>مجلة جامعة الملك عبدالعزيز-الآداب والعلوم الإن...</td>\n",
       "      <td>King Abdulaziz University Scientific Publishin...</td>\n",
       "      <td>[2017]</td>\n",
       "      <td>Abdelrahman</td>\n",
       "      <td>Alsolami</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10.1787/agr_outlook-2017-graph5-fr</td>\n",
       "      <td>Viande porcine : parts des régions dans la cro...</td>\n",
       "      <td></td>\n",
       "      <td>Organisation for Economic Co-Operation and Dev...</td>\n",
       "      <td>[2017, 7, 10]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10.4000/books.larhra.5210</td>\n",
       "      <td>La législation ecclésiale sous l’empereur Jose...</td>\n",
       "      <td>Droits antiromains xvie-xxie siècles</td>\n",
       "      <td>LARHRA</td>\n",
       "      <td>[2017]</td>\n",
       "      <td>Franz Xaver</td>\n",
       "      <td>Bischof</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    DOI  \\\n",
       "0   10.1136/annrheumdis-2017-eular.4238   \n",
       "1   10.1136/annrheumdis-2017-eular.4238   \n",
       "2   10.1136/annrheumdis-2017-eular.4238   \n",
       "3   10.1136/annrheumdis-2017-eular.4238   \n",
       "4   10.1136/annrheumdis-2017-eular.4238   \n",
       "5   10.1136/annrheumdis-2017-eular.4238   \n",
       "6   10.1136/annrheumdis-2017-eular.4238   \n",
       "7   10.1136/annrheumdis-2017-eular.4238   \n",
       "8   10.1136/annrheumdis-2017-eular.4238   \n",
       "9   10.1136/annrheumdis-2017-eular.4238   \n",
       "10  10.1136/annrheumdis-2017-eular.4238   \n",
       "11  10.1136/annrheumdis-2017-eular.4238   \n",
       "12      10.14445/22312803/ijctt-v47p113   \n",
       "13      10.14445/22312803/ijctt-v47p113   \n",
       "14      10.14445/22312803/ijctt-v47p113   \n",
       "15        10.1080/21695717.2017.1316452   \n",
       "16        10.1080/21695717.2017.1316452   \n",
       "17                   10.4197/art.25-2.6   \n",
       "18   10.1787/agr_outlook-2017-graph5-fr   \n",
       "19            10.4000/books.larhra.5210   \n",
       "\n",
       "                                                Title  \\\n",
       "0   SAT0241 Early response to belimumab in sle-rel...   \n",
       "1   SAT0241 Early response to belimumab in sle-rel...   \n",
       "2   SAT0241 Early response to belimumab in sle-rel...   \n",
       "3   SAT0241 Early response to belimumab in sle-rel...   \n",
       "4   SAT0241 Early response to belimumab in sle-rel...   \n",
       "5   SAT0241 Early response to belimumab in sle-rel...   \n",
       "6   SAT0241 Early response to belimumab in sle-rel...   \n",
       "7   SAT0241 Early response to belimumab in sle-rel...   \n",
       "8   SAT0241 Early response to belimumab in sle-rel...   \n",
       "9   SAT0241 Early response to belimumab in sle-rel...   \n",
       "10  SAT0241 Early response to belimumab in sle-rel...   \n",
       "11  SAT0241 Early response to belimumab in sle-rel...   \n",
       "12  Object Detection and Semantic Segmentation usi...   \n",
       "13  Object Detection and Semantic Segmentation usi...   \n",
       "14  Object Detection and Semantic Segmentation usi...   \n",
       "15  Relative role of heredity or training for musi...   \n",
       "16  Relative role of heredity or training for musi...   \n",
       "17  The impact of knowing the reasons of the Hadee...   \n",
       "18  Viande porcine : parts des régions dans la cro...   \n",
       "19  La législation ecclésiale sous l’empereur Jose...   \n",
       "\n",
       "                                      Container Title  \\\n",
       "0                                Poster Presentations   \n",
       "1                                Poster Presentations   \n",
       "2                                Poster Presentations   \n",
       "3                                Poster Presentations   \n",
       "4                                Poster Presentations   \n",
       "5                                Poster Presentations   \n",
       "6                                Poster Presentations   \n",
       "7                                Poster Presentations   \n",
       "8                                Poster Presentations   \n",
       "9                                Poster Presentations   \n",
       "10                               Poster Presentations   \n",
       "11                               Poster Presentations   \n",
       "12  International Journal of Computer Trends and T...   \n",
       "13  International Journal of Computer Trends and T...   \n",
       "14  International Journal of Computer Trends and T...   \n",
       "15                 Hearing, Balance and Communication   \n",
       "16                 Hearing, Balance and Communication   \n",
       "17  مجلة جامعة الملك عبدالعزيز-الآداب والعلوم الإن...   \n",
       "18                                                      \n",
       "19               Droits antiromains xvie-xxie siècles   \n",
       "\n",
       "                                            Publisher   Publish Date  \\\n",
       "0   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "1   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "2   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "3   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "4   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "5   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "6   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "7   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "8   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "9   BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "10  BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "11  BMJ Publishing Group Ltd and European League A...      [2017, 6]   \n",
       "12              Seventh Sense Research Group Journals             []   \n",
       "13              Seventh Sense Research Group Journals             []   \n",
       "14              Seventh Sense Research Group Journals             []   \n",
       "15                                 Informa UK Limited   [2017, 4, 3]   \n",
       "16                                 Informa UK Limited   [2017, 4, 3]   \n",
       "17  King Abdulaziz University Scientific Publishin...         [2017]   \n",
       "18  Organisation for Economic Co-Operation and Dev...  [2017, 7, 10]   \n",
       "19                                             LARHRA         [2017]   \n",
       "\n",
       "   Author First Name Author Last Name  Author Order  Referenced By  \n",
       "0                  L          Massaro             1              0  \n",
       "1                  F       Ceccarelli             2              0  \n",
       "2                 FR         Spinelli             3              0  \n",
       "3                  F          Morello             4              0  \n",
       "4                  C        Perricone             5              0  \n",
       "5                  F          Miranda             6              0  \n",
       "6                  S          Truglia             7              0  \n",
       "7                  V          Orefice             8              0  \n",
       "8                 IM       Rutigliano             9              0  \n",
       "9                  C       Alessandri            10              0  \n",
       "10                 G         Valesini            11              0  \n",
       "11                 F            Conti            12              0  \n",
       "12             R.Kar            thika             1              0  \n",
       "13                                                2              0  \n",
       "14        S.N Santha          lakshmi             3              0  \n",
       "15           Saransh             Jain             1              0  \n",
       "16     Harshan Kumar            H. S.             2              0  \n",
       "17       Abdelrahman         Alsolami             1              0  \n",
       "18                                                1              0  \n",
       "19       Franz Xaver          Bischof             1              0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "rows = 1000\n",
    "\n",
    "def get_articles_by_year(year, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows  # Adjust this number to change the number of articles per year\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    cursor = '*'\n",
    "\n",
    "    while cursor:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                break\n",
    "            \n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor', '')\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "years_to_pull = [2017,2018,2019,2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "\n",
    "for year in years_to_pull:\n",
    "    articles = get_articles_by_year(year, rows = rows)  # Adjust this number for the desired articles per year\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "for article in all_articles:\n",
    "    doi = article.get('DOI', '')\n",
    "    title = article.get('title', [''])[0]\n",
    "    container_title = article.get('container-title', [''])[0]\n",
    "    publisher = article.get('publisher', '')\n",
    "    publish_date = article.get('published-print', {}).get('date-parts', [[]])[0]\n",
    "    authors = article.get('author', [])\n",
    "    referenced_by = article.get('is-referenced-by-count', 0)\n",
    "\n",
    "    for order, author in enumerate(authors, start=1):\n",
    "        first_name = author.get('given', '')\n",
    "        last_name = author.get('family', '')\n",
    "        \n",
    "        data['DOI'].append(doi)\n",
    "        data['Title'].append(title)\n",
    "        data['Container Title'].append(container_title)\n",
    "        data['Publisher'].append(publisher)\n",
    "        data['Publish Date'].append(publish_date)\n",
    "        data['Author First Name'].append(first_name)\n",
    "        data['Author Last Name'].append(last_name)\n",
    "        data['Author Order'].append(order)\n",
    "        data['Referenced By'].append(referenced_by)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date', 'Author First Name', 'Author Last Name', 'Author Order', 'Referenced By']\n",
    "print(df.shape)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT BAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "rows = 1000  # Adjust this number for the desired articles per year\n",
    "\n",
    "# Function to fetch articles for a given year\n",
    "def get_articles_by_year(year, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows  \n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    cursor = '*'  # Initialize cursor for pagination\n",
    "\n",
    "    while cursor:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "            \n",
    "            if not items:\n",
    "                break\n",
    "            \n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor', '')  # Retrieve next cursor for pagination\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "    return articles\n",
    "\n",
    "# List of years to pull articles for\n",
    "years_to_pull = [2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "\n",
    "# Fetch articles for each year and add to the list\n",
    "for year in years_to_pull:\n",
    "    articles = get_articles_by_year(year, rows=rows)\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "# Initialize data dictionary to store extracted data\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "# Extract data from each article and populate the data dictionary\n",
    "for article in all_articles:\n",
    "    # ... (data extraction logic as before)\n",
    "    # Check 'published-print', 'published-online', and 'published' for date information\n",
    "    publish_date_parts = None\n",
    "    for key in ['published-print', 'published-online', 'published']:\n",
    "        publish_date_parts = article.get(key, {}).get('date-parts', [])\n",
    "        if publish_date_parts:\n",
    "            break\n",
    "\n",
    "    # If no publish date information is found, set it as an empty list\n",
    "    if not publish_date_parts:\n",
    "        publish_date_parts = [[]]\n",
    "\n",
    "    # Use the first available date as the 'Publish Date'\n",
    "    publish_date = publish_date_parts[0]\n",
    "\n",
    "    # ... (rest of the data extraction)\n",
    "\n",
    "    data['DOI'].append(doi)\n",
    "    data['Title'].append(title)\n",
    "    data['Container Title'].append(container_title)\n",
    "    data['Publisher'].append(publisher)\n",
    "    data['Publish Date'].append(publish_date)\n",
    "    data['Author First Name'].append(first_name)\n",
    "    data['Author Last Name'].append(last_name)\n",
    "    data['Author Order'].append(order)\n",
    "    data['Referenced By'].append(referenced_by)\n",
    "\n",
    "# Create a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date', 'Author First Name', 'Author Last Name', 'Author Order', 'Referenced By']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publish Date\n",
      "[2023]           243\n",
      "[2021]           226\n",
      "[2023, 3, 2]     212\n",
      "[2022]           211\n",
      "[2020]           200\n",
      "                ... \n",
      "[2020, 6, 3]       1\n",
      "[2021, 4, 19]      1\n",
      "[2021, 4, 15]      1\n",
      "[2020, 2, 11]      1\n",
      "[2021, 2, 2]       1\n",
      "Name: count, Length: 709, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Grouping the DataFrame by 'Publish Date' year and counting unique DOIs\n",
    "unique_doi_counts = df['Publish Date'].value_counts(dropna=False)\n",
    "\n",
    "print(unique_doi_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null counts in each column:\n",
      "DOI                  0\n",
      "Title                0\n",
      "Container Title      0\n",
      "Publisher            0\n",
      "Publish Date         0\n",
      "Author First Name    0\n",
      "Author Last Name     0\n",
      "Author Order         0\n",
      "Referenced By        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have already created the DataFrame 'df' with the metadata\n",
    "\n",
    "# Counting the number of null values in each column\n",
    "null_counts = df.isna().sum()\n",
    "\n",
    "print(\"Null counts in each column:\")\n",
    "print(null_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 9)\n",
      "                            DOI  \\\n",
      "0  10.2991/978-2-494069-95-4_49   \n",
      "1  10.2991/978-2-494069-95-4_49   \n",
      "2  10.2991/978-2-494069-95-4_49   \n",
      "3  10.2991/978-2-494069-95-4_49   \n",
      "4  10.2991/978-2-494069-95-4_49   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Intercultural Communicative Competence (ICC): ...   \n",
      "1  Intercultural Communicative Competence (ICC): ...   \n",
      "2  Intercultural Communicative Competence (ICC): ...   \n",
      "3  Intercultural Communicative Competence (ICC): ...   \n",
      "4  Intercultural Communicative Competence (ICC): ...   \n",
      "\n",
      "                             Container Title            Publisher  \\\n",
      "0  Journal of Language Teaching and Research  Academy Publication   \n",
      "1  Journal of Language Teaching and Research  Academy Publication   \n",
      "2  Journal of Language Teaching and Research  Academy Publication   \n",
      "3  Journal of Language Teaching and Research  Academy Publication   \n",
      "4  Journal of Language Teaching and Research  Academy Publication   \n",
      "\n",
      "    Publish Date Author First Name Author Last Name  Author Order  \\\n",
      "0   [2020, 6, 1]          Narathip      Thumawongsa             2   \n",
      "1         [2020]          Narathip      Thumawongsa             2   \n",
      "2  [2020, 9, 18]          Narathip      Thumawongsa             2   \n",
      "3         [2020]          Narathip      Thumawongsa             2   \n",
      "4  [2020, 3, 23]          Narathip      Thumawongsa             2   \n",
      "\n",
      "   Referenced By  \n",
      "0              0  \n",
      "1              0  \n",
      "2              0  \n",
      "3              0  \n",
      "4              0  \n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved as 'D:/School/GitHub/SMU_MSDS_Capstone\\metadata.csv'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Specify the directory and file name for the CSV file\n",
    "csv_dir = 'D:/School/GitHub/SMU_MSDS_Capstone'  # Change this to the desired directory\n",
    "csv_file_name = 'metadata.csv'\n",
    "csv_file_path = os.path.join(csv_dir, csv_file_name)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Save the DataFrame as a CSV file with UTF-8 encoding\n",
    "df.to_csv(csv_file_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"DataFrame saved as '{csv_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT DEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Articles: 100%|██████████| 4000/4000 [00:00<00:00, 1998239.16it/s]\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:89: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorFirstName_{j + 1}'] = author.get('given', '')\n",
      "C:\\Users\\raag7\\AppData\\Local\\Temp\\ipykernel_12820\\2823426116.py:90: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[f'AuthorLastName_{j + 1}'] = author.get('family', '')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical DataFrame saved as 'metadata_vertical.csv'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Adjust this number for the desired articles per year\n",
    "rows_per_request = 1000\n",
    "\n",
    "def get_articles_by_year(year, rows=rows_per_request, cursor=None):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows,\n",
    "        'cursor': cursor  # Include the cursor for pagination\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor')\n",
    "            if not cursor:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "        # Sleep for a short duration to stay within rate limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return articles\n",
    "\n",
    "years_to_pull = [2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "\n",
    "# Loop through each year\n",
    "for year in years_to_pull:\n",
    "    cursor = None  # Initialize cursor for pagination\n",
    "    # Loop until all pages of articles are retrieved\n",
    "    while True:\n",
    "        articles = get_articles_by_year(year, rows=rows_per_request, cursor=cursor)\n",
    "        if not articles:\n",
    "            break\n",
    "        all_articles.extend(articles)\n",
    "        # Update the cursor for the next page of articles\n",
    "        cursor = articles[-1].get('message', {}).get('next-cursor')\n",
    "        if not cursor:\n",
    "            break\n",
    "        # Sleep for a short duration to stay within rate limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Initialize data dictionary to store extracted data\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "# Extract data from each article and populate the data dictionary\n",
    "for article in tqdm(all_articles, desc=\"Processing Articles\"):\n",
    "    # ... (data extraction logic as before)\n",
    "\n",
    "    data['DOI'].append(doi)\n",
    "    data['Title'].append(title)\n",
    "    data['Container Title'].append(container_title)\n",
    "    data['Publisher'].append(publisher)\n",
    "    data['Publish Date'].append(publish_date)\n",
    "    data['Referenced By'].append(referenced_by)\n",
    "\n",
    "# Create a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract authors as a list of dictionaries\n",
    "authors_list = [article.get('author', []) for article in all_articles]\n",
    "author_columns = []\n",
    "for i, authors in enumerate(authors_list):\n",
    "    author_data = {\n",
    "        f'AuthorFirstName_{j + 1}': author.get('given', ''),\n",
    "        f'AuthorLastName_{j + 1}': author.get('family', '')\n",
    "    }\n",
    "    author_columns.append(author_data)\n",
    "\n",
    "# Concatenate all author columns at once\n",
    "author_df = pd.concat([pd.DataFrame(ac) for ac in author_columns], axis=1)\n",
    "\n",
    "# Reorganize the DataFrame to vertical format\n",
    "vertical_data = {\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': []\n",
    "}\n",
    "for i in range(1, max(len(authors) for authors in authors_list) + 1):\n",
    "    vertical_data['Author First Name'].extend(author_df[f'AuthorFirstName_{i}'])\n",
    "    vertical_data['Author Last Name'].extend(author_df[f'AuthorLastName_{i}'])\n",
    "    vertical_data['Author Order'].extend([i] * len(df))\n",
    "\n",
    "vertical_df = pd.DataFrame(vertical_data)\n",
    "\n",
    "# Save the vertical DataFrame as a CSV file (adjust the file path accordingly)\n",
    "csv_file_path = 'metadata_vertical.csv'\n",
    "vertical_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"Vertical DataFrame saved as '{csv_file_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT DEAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Adjust this number for the desired articles per year\n",
    "rows_per_request = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_by_year(year, rows=rows_per_request, cursor=None):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows,\n",
    "        'cursor': cursor  # Include the cursor for pagination\n",
    "    }\n",
    "\n",
    "    articles = []\n",
    "    while True:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('message', {}).get('items', [])\n",
    "\n",
    "            if not items:\n",
    "                break\n",
    "\n",
    "            articles.extend(items)\n",
    "            cursor = data.get('message', {}).get('next-cursor')\n",
    "            if not cursor:\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Request failed with status code: {response.status_code}\")\n",
    "            print(response.content)\n",
    "            break\n",
    "\n",
    "        # Sleep for a short duration to stay within rate limits\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_to_pull = [2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "\n",
    "# Loop through each year\n",
    "for year in years_to_pull:\n",
    "    cursor = None  # Initialize cursor for pagination\n",
    "    # Loop until all pages of articles are retrieved\n",
    "    while True:\n",
    "        articles = get_articles_by_year(year, rows=rows_per_request, cursor=cursor)\n",
    "        if not articles:\n",
    "            break\n",
    "        all_articles.extend(articles)\n",
    "        # Update the cursor for the next page of articles\n",
    "        cursor = articles[-1].get('message', {}).get('next-cursor')\n",
    "        if not cursor:\n",
    "            break\n",
    "        # Sleep for a short duration to stay within rate limits\n",
    "        time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Articles: 100%|██████████| 4000/4000 [00:00<00:00, 1998477.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Initialize data dictionary to store extracted data\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "# Extract data from each article and populate the data dictionary\n",
    "for article in tqdm(all_articles, desc=\"Processing Articles\"):\n",
    "    # ... (data extraction logic as before)\n",
    "\n",
    "    data['DOI'].append(doi)\n",
    "    data['Title'].append(title)\n",
    "    data['Container Title'].append(container_title)\n",
    "    data['Publisher'].append(publisher)\n",
    "    data['Publish Date'].append(publish_date)\n",
    "    data['Referenced By'].append(referenced_by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract authors as a list of dictionaries\n",
    "authors_list = [article.get('author', []) for article in all_articles]\n",
    "author_columns = []\n",
    "for i, authors in enumerate(authors_list):\n",
    "    author_data = {\n",
    "        f'AuthorFirstName_{j + 1}': author.get('given', ''),\n",
    "        f'AuthorLastName_{j + 1}': author.get('family', '')\n",
    "    }\n",
    "    author_columns.append(author_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DOI</th>\n",
       "      <th>Title</th>\n",
       "      <th>Container Title</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Publish Date</th>\n",
       "      <th>Author First Name</th>\n",
       "      <th>Author Last Name</th>\n",
       "      <th>Author Order</th>\n",
       "      <th>Referenced By</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.2991/978-2-494069-95-4_49</td>\n",
       "      <td>Intercultural Communicative Competence (ICC): ...</td>\n",
       "      <td>Journal of Language Teaching and Research</td>\n",
       "      <td>Academy Publication</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>Narathip</td>\n",
       "      <td>Thumawongsa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.2991/978-2-494069-95-4_49</td>\n",
       "      <td>Intercultural Communicative Competence (ICC): ...</td>\n",
       "      <td>Journal of Language Teaching and Research</td>\n",
       "      <td>Academy Publication</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>Narathip</td>\n",
       "      <td>Thumawongsa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.2991/978-2-494069-95-4_49</td>\n",
       "      <td>Intercultural Communicative Competence (ICC): ...</td>\n",
       "      <td>Journal of Language Teaching and Research</td>\n",
       "      <td>Academy Publication</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>Narathip</td>\n",
       "      <td>Thumawongsa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.2991/978-2-494069-95-4_49</td>\n",
       "      <td>Intercultural Communicative Competence (ICC): ...</td>\n",
       "      <td>Journal of Language Teaching and Research</td>\n",
       "      <td>Academy Publication</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>Narathip</td>\n",
       "      <td>Thumawongsa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.2991/978-2-494069-95-4_49</td>\n",
       "      <td>Intercultural Communicative Competence (ICC): ...</td>\n",
       "      <td>Journal of Language Teaching and Research</td>\n",
       "      <td>Academy Publication</td>\n",
       "      <td>[2023]</td>\n",
       "      <td>Narathip</td>\n",
       "      <td>Thumawongsa</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            DOI  \\\n",
       "0  10.2991/978-2-494069-95-4_49   \n",
       "1  10.2991/978-2-494069-95-4_49   \n",
       "2  10.2991/978-2-494069-95-4_49   \n",
       "3  10.2991/978-2-494069-95-4_49   \n",
       "4  10.2991/978-2-494069-95-4_49   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Intercultural Communicative Competence (ICC): ...   \n",
       "1  Intercultural Communicative Competence (ICC): ...   \n",
       "2  Intercultural Communicative Competence (ICC): ...   \n",
       "3  Intercultural Communicative Competence (ICC): ...   \n",
       "4  Intercultural Communicative Competence (ICC): ...   \n",
       "\n",
       "                             Container Title            Publisher  \\\n",
       "0  Journal of Language Teaching and Research  Academy Publication   \n",
       "1  Journal of Language Teaching and Research  Academy Publication   \n",
       "2  Journal of Language Teaching and Research  Academy Publication   \n",
       "3  Journal of Language Teaching and Research  Academy Publication   \n",
       "4  Journal of Language Teaching and Research  Academy Publication   \n",
       "\n",
       "  Publish Date Author First Name Author Last Name  Author Order  Referenced By  \n",
       "0       [2023]          Narathip      Thumawongsa             2              0  \n",
       "1       [2023]          Narathip      Thumawongsa             2              0  \n",
       "2       [2023]          Narathip      Thumawongsa             2              0  \n",
       "3       [2023]          Narathip      Thumawongsa             2              0  \n",
       "4       [2023]          Narathip      Thumawongsa             2              0  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries for authors\n",
    "authors_data = []\n",
    "\n",
    "for authors in authors_list:\n",
    "    author_data = {}\n",
    "    for j, author in enumerate(authors, start=1):\n",
    "        author_data[f'AuthorFirstName_{j}'] = author.get('given', '')\n",
    "        author_data[f'AuthorLastName_{j}'] = author.get('family', '')\n",
    "    authors_data.append(author_data)\n",
    "\n",
    "# Create a DataFrame from the list of author dictionaries\n",
    "author_df = pd.DataFrame(authors_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AuthorFirstName_1</th>\n",
       "      <th>AuthorLastName_1</th>\n",
       "      <th>AuthorFirstName_2</th>\n",
       "      <th>AuthorLastName_2</th>\n",
       "      <th>AuthorFirstName_3</th>\n",
       "      <th>AuthorLastName_3</th>\n",
       "      <th>AuthorFirstName_4</th>\n",
       "      <th>AuthorLastName_4</th>\n",
       "      <th>AuthorFirstName_5</th>\n",
       "      <th>AuthorLastName_5</th>\n",
       "      <th>...</th>\n",
       "      <th>AuthorFirstName_92</th>\n",
       "      <th>AuthorLastName_92</th>\n",
       "      <th>AuthorFirstName_93</th>\n",
       "      <th>AuthorLastName_93</th>\n",
       "      <th>AuthorFirstName_94</th>\n",
       "      <th>AuthorLastName_94</th>\n",
       "      <th>AuthorFirstName_95</th>\n",
       "      <th>AuthorLastName_95</th>\n",
       "      <th>AuthorFirstName_96</th>\n",
       "      <th>AuthorLastName_96</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilene Cirenia</td>\n",
       "      <td>Escamilla Ortiz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jessé Rafael Bento</td>\n",
       "      <td>Lima</td>\n",
       "      <td>Clayton dos Santos</td>\n",
       "      <td>Silva</td>\n",
       "      <td>Romário Guimarães Verçosa</td>\n",
       "      <td>Araújo</td>\n",
       "      <td>Jonas Olimpio de Lima</td>\n",
       "      <td>Silva</td>\n",
       "      <td>Arlla Katherine Xavier</td>\n",
       "      <td>Lima</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jing</td>\n",
       "      <td>Gu</td>\n",
       "      <td>Qiaotong</td>\n",
       "      <td>Pang</td>\n",
       "      <td>Jinzhi</td>\n",
       "      <td>Ding</td>\n",
       "      <td>Runsheng</td>\n",
       "      <td>Yin</td>\n",
       "      <td>Yuanhe</td>\n",
       "      <td>Yang</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    AuthorFirstName_1 AuthorLastName_1   AuthorFirstName_2 AuthorLastName_2  \\\n",
       "0                 NaN              NaN                 NaN              NaN   \n",
       "1     Abilene Cirenia  Escamilla Ortiz                 NaN              NaN   \n",
       "2                 NaN              NaN                 NaN              NaN   \n",
       "3  Jessé Rafael Bento             Lima  Clayton dos Santos            Silva   \n",
       "4                Jing               Gu            Qiaotong             Pang   \n",
       "\n",
       "           AuthorFirstName_3 AuthorLastName_3      AuthorFirstName_4  \\\n",
       "0                        NaN              NaN                    NaN   \n",
       "1                        NaN              NaN                    NaN   \n",
       "2                        NaN              NaN                    NaN   \n",
       "3  Romário Guimarães Verçosa           Araújo  Jonas Olimpio de Lima   \n",
       "4                     Jinzhi             Ding               Runsheng   \n",
       "\n",
       "  AuthorLastName_4       AuthorFirstName_5 AuthorLastName_5  ...  \\\n",
       "0              NaN                     NaN              NaN  ...   \n",
       "1              NaN                     NaN              NaN  ...   \n",
       "2              NaN                     NaN              NaN  ...   \n",
       "3            Silva  Arlla Katherine Xavier             Lima  ...   \n",
       "4              Yin                  Yuanhe             Yang  ...   \n",
       "\n",
       "  AuthorFirstName_92 AuthorLastName_92 AuthorFirstName_93 AuthorLastName_93  \\\n",
       "0                NaN               NaN                NaN               NaN   \n",
       "1                NaN               NaN                NaN               NaN   \n",
       "2                NaN               NaN                NaN               NaN   \n",
       "3                NaN               NaN                NaN               NaN   \n",
       "4                NaN               NaN                NaN               NaN   \n",
       "\n",
       "  AuthorFirstName_94 AuthorLastName_94 AuthorFirstName_95 AuthorLastName_95  \\\n",
       "0                NaN               NaN                NaN               NaN   \n",
       "1                NaN               NaN                NaN               NaN   \n",
       "2                NaN               NaN                NaN               NaN   \n",
       "3                NaN               NaN                NaN               NaN   \n",
       "4                NaN               NaN                NaN               NaN   \n",
       "\n",
       "  AuthorFirstName_96 AuthorLastName_96  \n",
       "0                NaN               NaN  \n",
       "1                NaN               NaN  \n",
       "2                NaN               NaN  \n",
       "3                NaN               NaN  \n",
       "4                NaN               NaN  \n",
       "\n",
       "[5 rows x 192 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array length 384000 does not match index length 4000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     vertical_data[\u001b[39m'\u001b[39m\u001b[39mAuthor Last Name\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mextend(author_df[\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAuthorLastName_\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m])\n\u001b[0;32m     10\u001b[0m     vertical_data[\u001b[39m'\u001b[39m\u001b[39mAuthor Order\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mextend([i] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(df))\n\u001b[1;32m---> 12\u001b[0m vertical_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(vertical_data)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    705\u001b[0m     )\n\u001b[0;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:115\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    113\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    116\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\raag7\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:668\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    663\u001b[0m     \u001b[39mif\u001b[39;00m lengths[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m    664\u001b[0m         msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    665\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39marray length \u001b[39m\u001b[39m{\u001b[39;00mlengths[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m does not match index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    666\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mlength \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    667\u001b[0m         )\n\u001b[1;32m--> 668\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    669\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m default_index(lengths[\u001b[39m0\u001b[39m])\n",
      "\u001b[1;31mValueError\u001b[0m: array length 384000 does not match index length 4000"
     ]
    }
   ],
   "source": [
    "# Reorganize the DataFrame to vertical format\n",
    "vertical_data = {\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': []\n",
    "}\n",
    "for i in range(1, max(len(authors) for authors in authors_list) + 1):\n",
    "    vertical_data['Author First Name'].extend(author_df[f'AuthorFirstName_{i}'])\n",
    "    vertical_data['Author Last Name'].extend(author_df[f'AuthorLastName_{i}'])\n",
    "    vertical_data['Author Order'].extend([i] * len(df))\n",
    "\n",
    "vertical_df = pd.DataFrame(vertical_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertical DataFrame saved as 'metadata_vertical.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the vertical DataFrame as a CSV file (adjust the file path accordingly)\n",
    "csv_file_path = 'metadata_vertical.csv'\n",
    "vertical_df.to_csv(csv_file_path, index=False)\n",
    "print(f\"Vertical DataFrame saved as '{csv_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTEMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique DOIs after 2020: 1000\n",
      "Total unique DOIs after 2021: 2000\n",
      "Total unique DOIs after 2022: 3000\n",
      "Total unique DOIs after 2023: 4000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "rows = 1000  # Adjust this number for the desired articles per year\n",
    "\n",
    "# Function to fetch articles for a given year and cursor\n",
    "def get_articles_by_year(year, cursor=None, rows=rows):\n",
    "    base_url = 'https://api.crossref.org/works'\n",
    "    params = {\n",
    "        'filter': f'from-pub-date:{year}-01-01,until-pub-date:{year}-12-31',\n",
    "        'sort': 'relevance',\n",
    "        'rows': rows,\n",
    "        'cursor': cursor\n",
    "    }\n",
    "\n",
    "    response = requests.get(base_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        items = data.get('message', {}).get('items', [])\n",
    "        next_cursor = data.get('message', {}).get('next-cursor', None)\n",
    "        return items, next_cursor\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        print(response.content)\n",
    "        return [], None\n",
    "\n",
    "# List of years to pull articles for\n",
    "years_to_pull = [2020, 2021, 2022, 2023]\n",
    "all_articles = []\n",
    "unique_dois = set()  # To track unique DOIs\n",
    "\n",
    "# Fetch articles for each year and add to the list\n",
    "for year in years_to_pull:\n",
    "    cursor = None\n",
    "    while True:\n",
    "        articles, cursor = get_articles_by_year(year, cursor, rows=rows)\n",
    "        if not articles:\n",
    "            break\n",
    "        for article in articles:\n",
    "            doi = article.get('DOI', '')\n",
    "            if doi not in unique_dois:\n",
    "                all_articles.append(article)\n",
    "                unique_dois.add(doi)\n",
    "        if cursor is None:\n",
    "            break\n",
    "    print(f\"Total unique DOIs after {year}: {len(unique_dois)}\")\n",
    "\n",
    "# Initialize data dictionary to store extracted data\n",
    "data = {\n",
    "    'DOI': [],\n",
    "    'Title': [],\n",
    "    'Container Title': [],\n",
    "    'Publisher': [],\n",
    "    'Publish Date': [],\n",
    "    'Author First Name': [],\n",
    "    'Author Last Name': [],\n",
    "    'Author Order': [],\n",
    "    'Referenced By': []\n",
    "}\n",
    "\n",
    "# Extract data from each article and populate the data dictionary\n",
    "for article in all_articles:\n",
    "    # ... (data extraction logic as before)\n",
    "\n",
    "    # ... (rest of the data extraction)\n",
    "\n",
    "    data['DOI'].append(doi)\n",
    "    data['Title'].append(title)\n",
    "    data['Container Title'].append(container_title)\n",
    "    data['Publisher'].append(publisher)\n",
    "    data['Publish Date'].append(publish_date)\n",
    "    data['Author First Name'].append(first_name)\n",
    "    data['Author Last Name'].append(last_name)\n",
    "    data['Author Order'].append(order)\n",
    "    data['Referenced By'].append(referenced_by)\n",
    "\n",
    "# Create a DataFrame from the data dictionary\n",
    "df = pd.DataFrame(data)\n",
    "df.columns = ['DOI', 'Title', 'Container Title', 'Publisher', 'Publish Date', 'Author First Name', 'Author Last Name', 'Author Order', 'Referenced By']\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('article_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
